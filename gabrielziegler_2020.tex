\documentclass[a4paper, 12pt]{fga}

% |--- Títulos, autor, banca ---|----------------------{{{
\title{Context-Dependent Probabilistic Prior Information Strategy for MRI Reconstruction} % título na forma principal
\tituloficha{Context-Dependent Probabilistic Prior Information Strategy for MRI Reconstruction,\\\,[Federal District], 2021.} % título na forma a constar da ficha
                                                  % catalográfica (incluir mudanças de
                                                  % linha usando \\ quando necessário)
% \titulocapaA{Compressed Sensing with Gaussian Probabilistic}
% \titulocapaB{Prior Information For}
% \titulocapaC{Magnetic Resonance Image Reconstruction}
\titulocapaA{Context-Dependent Probabilistic}
\titulocapaB{Prior Information Strategy for MRI Reconstruction}
\titulocapaC{}

\titulofichadois{Context-Dependent Probabilistic Prior Information Strategy for MRI Reconstruction}
\author{~Gabriel Gomes Ziegler}
\nomeinvertido{Ziegler, Gabriel}
\orientador{~Cristiano Jacques Miosso, PhD}
\coorientador{~Davi Benevides Gusmão, MSc}
\data{May 2021}
\ano{2021}
\areaum{Compressed Sensing}
\areadois{Magnetic Resonance Image Reconstruction}
\areatres{\emph{Prior Information}}
\areaquatro{Deep Learning}
\endereco{gabrielziegler3@gmail.com}
\cep{CEP 72.444\-240}

\membrobancainterno{~Prof. Adson Ferreira da Rocha, PhD}

\membrobancaexterno{~Felipe Batista da Silva, PhD}
%---}}}

% |--- Bibliotecas utilizadas ---|----------------------{{{
\usepackage[margin=1in]{geometry}
\usepackage{setspace}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage[latin1]{inputenc}
\usepackage{enumerate}
\usepackage{xfrac}
\usepackage{color, colortbl}
\usepackage{placeins}
\usepackage[graphicx]{realboxes}
\usepackage{float}
\usepackage{subfig}
\usepackage{amsmath, amssymb}
\usepackage{rotating}
\usepackage{indentfirst}
\usepackage{multicol, blindtext, graphicx}
\usepackage[margin=0.40in,font=small,labelfont=bf,labelsep=period]{caption}
\usepackage{hyperref}
\usepackage[autostyle]{csquotes}
% \usepackage{subcaption} %to have subfigures available
\hypersetup{
    colorlinks,
    citecolor=blue,
    filecolor=blue,
    linkcolor=black,
    urlcolor=blue
}
\captionsetup[subfigure]{labelformat=empty}


%
%---}}}

% |- Formato de referências (use apenas uma das 2 linhas seguintes; comente a outra) -|-{{{
\newcommand{\formatobibliografia}{numero}
%\newcommand{\formatobibliografia}{autorano}

\ifthenelse{\equal{\formatobibliografia}{numero}}{
\bibliographystyle{unsrt}
}
{}

\ifthenelse{\equal{\formatobibliografia}{autorano}}{
\usepackage{apalike}
\bibliographystyle{apalike}
}
{}
%---}}}

% |--- Espaçamento, configuração de título de seções ---|----------------------{{{
\onehalfspacing

\makeatletter
\renewcommand{\section}{\@startsection
{section}
{1}
{0mm}
{-\baselineskip}
{0.5\baselineskip}
{\large\bf}}
\makeatother

\makeatletter
\renewcommand{\subsection}{\@startsection
{subsection}
{2}
{0mm}
{-\baselineskip}
{0.5\baselineskip}
{\bf\sffamily}}
\makeatother

\makeatletter
\renewcommand{\subsubsection}{\@startsection
{subsubsection}
{3}
{0mm}
{-\baselineskip}
{0.5\baselineskip}
{\bf\sffamily}}
\makeatother

\setlength{\parindent}{20pt}
\setlength{\parskip}{12pt}
\newcommand{\spaceinitialsname}{0.4mm}
\newcommand{\porcento}{\scalebox{0.5}{~}\scalebox{0.9}{\%}}
\newcommand{\scanner}{\emph{scanner}}
\newcommand{\scanners}{\emph{scanners}}
\newcommand{\cmcubico}{${\textrm{cm}^{\scalebox{0.7}{3} }}$}
\setcounter{secnumdepth}{3}
%\setcounter{tocdepth}{3}
%---}}}

% |--- Comandos especiais ---|----------------------{{{
\newcommand{\cmquad}{${\textrm{cm}^{\scalebox{0.7}{2}} }$}
\newcommand{\mmquad}{${\textrm{mm}^{\scalebox{0.7}{2}} }$}
\newcommand{\gcmquad}{${\textrm{g}}/{\textrm{cm}^{\scalebox{0.7}{2}} }$}
\newcommand{\subsecref}[1]{Section~\ref{#1}}
\newcommand{\figref}[1]{Figure~\ref{#1}}
\newcommand{\etal}{\emph{et~al.}}
\newcommand{\Jawsonly}{{\emph{Jaws-Only}} }
\newcommand{\jawsonly}{{\emph{jaws-only}} }
\newcommand{\software}{\emph{software}}
\newcommand{\Section}[1]{\section{\textbf{#1} }}
\newcommand{\Sectionlabel}[2]{\section{\textbf{#1}\label{#2} }}
\newcommand{\percentagesignscale}{0.9}
\newcommand{\subsubsubsection}[1]{\vspace{16pt}\noindent\textbf{#1}\\[12pt]}
\newcommand{\commenttext}[1]{}
%---}}}

% |--- Diretório(s) com figuras (se desejar, inclua subdiretórios) ---|-------------{{{
\graphicspath{{figuras/}}
%---}}}

% |--- Lista de palavras que não podem ser separadas em sílabas ---|------------------{{{
\hyphenation{development results Commissioning possibility Philadelphia Devic Calculations Calculation}
%---}}}

\def\changemargin#1#2{\list{}{\rightmargin#2\leftmargin#1}\item[]}
\let\endchangemargin=\endlist
% |--- Texto principal ---|----------------------{{{
\begin{document}

\sloppy

\maketitle

% |--- Epígrafe, dedicatória ---|----------------------{{{
% Se desejar uma epígrafe, remova o % do início das próximas linhas (até ==============)
%\clearpage
%\hspace{1mm}
%
%\vfill
%
%\hspace{1mm}
%
%\begin{center}
%\emph{Epígrafe} \\
%Autor da epígrafe
%\end{center}
%
%\hspace{1mm}
%
%\vfill
%
%\hspace{1mm}
% ==============

% Se desejar uma dedicatória, remova o % do início das próximas linhas (até ==============)
%\clearpage
%\hspace{1mm}
%
%\vfill
%
%\begin{flushright}
%\begin{itshape}
%Texto da dedicatória.
%\end{itshape}
%\end{flushright}
% ==============
%---}}}

% |--- Agradecimentos ---|----------------------{{{
% Se desejar incluir agradecimentos, remova o % do início das próximas linhas (até ==============)
\clearpage
\noindent{\bfseries{\maiusc{\large Agradecimentos}} }

A execução de toda a pesquisa e desenvolvimento da proposta de um método inovador contido neste trabalho não foi uma tarefa fácil, e na verdade foi um grande desafio acadêmico e pessoal que eu me propus a desenvolver, uma vez que se tratava de uma área completamente nova para mim.
Particularmente, entendo que este desafio só foi possível diante de várias condições que tive durante meu período de pesquisa, condições essas que foram proporcionadas por minha mãe Leninha que sempre me deu as melhores condições na vida e todo o suporte necessário para que eu estivesse focado, a fim de entregar o trabalho proposto.
Muito obrigado por sempre me dar suporte e estar ao meu lado, mãe!

Agradeço também todos os meus amigos e amigas que estiveram perto acompanhando este trabalho e torcendo por mim para que eu conseguisse entregar com êxito essa última atividade na UnB.
A presença dessas pessoas tornou o processo mais leve e com certeza foram fundamentais para uma execução com satisfação no processo de desenvolvimento e não só na entrega, o que para mim é o mais importante.

Em especial, agradeço o meu amigo Davi Gusmão por estar do meu lado nessa pesquisa como co-orientador e fora dela como amigo que sempre esteve disponível para me ajudar e para discutir ideias sobre trabalhos acadêmicos e profissionais.
Gostaria também de agradecer a honra que tive de ter os professores Dr. Adson Rocha e Dr. Felipe Silva que fizeram parte da minha banca e são duas figuras grandiosamente conceituadas e respeitadas na área de processamento de sinais e de MRI.
A contribuição dos senhores foi de grande importância para o fechamento do meu trabalho. Obrigado!

Por último, gostaria de agradecer de forma especial o meu orientador professor Dr. Cristiano Miosso que durante todo esse período foi um gigante  como professor, pesquisador, orientador e acima de tudo como pessoa. 
Fiquei extremamente feliz de ter a oportunidade de trabalhar com o Cristiano e sinto que fui agraciado de ter sido orientando dele durante esse ano que tantas coisas aconteceram.
Professor Cristiano trabalhou comigo durante a pandemia, mesmo em momentos que não estavamos oficialmente matriculados na UnB, ele sempre esteve extremamente atento ao meu aprendizado e em como eu iria aplicar os conceitos que me passava durante a orientação.
É um professor excepcional que superou e muito minhas expectativas para um orientador, além de me mostrar a todo momento ser uma pessoa de caráter muito íntegro durante as discussões e sempre me ensinando não só conteúdos técnicos, mas também sobre situações e observações importantes para ser uma pessoa boa e honesta.
Portanto, por contemplar tantos conhecimentos técnicos e ainda se manter um homem de enorme humildade e princípios, sinto que não podia ter sido orientado por alguém melhor.
Meu mais sincero obrigado, professor Cristiano! Você teve uma contribuição \textbf{gigante} na minha formação.


% \vspace{24pt} Agradecimentos


\noindent
\clearpage
% ==============
%---}}}

% |--- Resumo e Abstract ---|----------------------{{{
\newgeometry{bottom=0.8in, top=0.9in, left=0.9in, right=0.9in}

\noindent{\bfseries{\maiusc{\large Resumo}} }

A obtenção de imagens de ressonância magnética de alta qualidade é uma tarefa árdua devido a maneira de obtenção do sinal pelas máquinas de ressonância magnética e pela complexidade dos tecidos analisados, tornando praticamente impossível a obtenção de todas as medidas do objeto de estudo em um exame de breve execução.
Para lidar com este problema, diversas técnicas foram aplicadas para gerar imagens de alta qualidade com quantidades menores de medidas.
Entre as técnicas usadas, \ac{CS} tem sido a técnica com melhores resultados e mais aprofundada no contexto de reconstrução de imagens de ressonância magnética nas últimas duas décadas.
Por sua alta capacidade de lidar com sinais com representações esparsas, \ac{CS} é capaz de reconstruir imagens de ressonância magnética com altíssima qualidade com proporções de medidas muito menores de amostragens, como 15\%, 20\% do total de medidas contidas na imagem utilizando técnicas de subamostragem.

O desempenho do \ac{CS} evoluiu com as contribuições de uso de filtros esparsificantes e com o uso de informação à priori~\cite{miosso_phd, miosso_compressive_2009, miosso_prefiltering_2009}, entre outras aplicações que foram desenvolvidas seguindo a mesma linha de aplicação como por exemplo o uso de informação à priori de forma não determinística~\cite{daniel_msc}.
A extensa aplicação do \ac{CS} dentro do contexto de exames de ressonância magnética foi uma das grandes motivações de apresentarmos nesse trabalho uma técnica inédita e que gera resultados promissores com reconstruções melhores nas métricas avaliadas e com tempo de execução inferior ao de algoritmos que utilizam informação à priori determinística.
Nossa técnica é nomeada CoDePPI, do inglês \textit{Informação à Priori Probabilística Dependente do Contexto}, e traz uma característica nova em termos de informação à priori que é o entendimento que regiões diferentes de um objeto variam em termos de movimento de maneiras diferentes em um exame de ressonância dinâmico.
Isto é, em um exame dinâmico de ressonância cardíaco a região do coração obviamente vai estar se movendo entre frames em uma certa velocidade, mas outras regiões do corpo do objeto de estudo também se movimentam e com velocidades diferentes.
Levando essa informação de contexto de movimento em regiões diferentes da imagem, propomos neste trabalho utilizar uma técnica não determinística de informação à priori com diferentes variâncias para cada região de acordo com o movimento.
Dessa forma, cada região é representada por diferentes níveis de confiança de que aqueles pontos extraídos pelo algoritmo de informação a priori fazem parte de uma posição suporte (posições de píxeis que se manteriam estáticos entre frames).

Assim, desenvolvemos uma série de experimentos a fim de levantar conhecimento sobre as técnicas utilizadas em \ac{CS} para reconstrução de imagens utilizando técnicas de pré-filtragem e de informação à priori, e posteriormente avaliamos os resultados de cada técnica entre elas para diferentes modalidades de imagens de ressonância magnética e também utilizando diferentes maneiras de subamostragem para explorar como os resultados variam em cada situação.
Nos experimentos são levadas em consideração imagens simples como fantomas Shepp-Logan, cortes sagitais da cabeça e um estudo de caso de ressonância dinâmica de corte sagital do tórax.

Os resultados dos nossos experimentos mostram que o uso de pré-filtragem e de informação à priori são muito vantajosos para reconstrução de imagens com a teoria de \ac{CS} e são preferíveis à não utilização dos mesmos.
Além disso, nossos testes com o método proposto CoDePPI revela que o uso da nossa metodologia de extração de informação à priori é mais robusta e gera melhores resultados quando comparada com as outras técnicas para exames de ressonância magnética dinâmica.
Não foram feitos experimentos com outras modalidades, mas os resultados nos levam a crer que essa técnica também geraria os melhores resultados em outras situações como extração de informação à priori entre frames de uma ressonância axial do crânio que compartilha várias posições suportes entre frames.

Durante a execução desta pesquisa, nós também desenvolvemos outras hipóteses de melhorias para o processo de extração de informação a priori análogas à técnica do CoDePPI com uso de técnicas mais recentes de aprendizado de máquina.
Dentro dessas hipóteses, duas se destacam: a possibilidade da utilização de algoritmos generativos como as \ac{GAN}s para geração de pontos de posição suporte de forma automática e a utilização de técnicas de visão computacional para segmentação automática das regiões com movimentos diferentes que é utilizada na aplicação do CoDePPI.
Assim, sendo duas possibilidades de melhoria no contexto do nosso algoritmo apresentado, trouxemos nesta pesquisa os fundamentos teóricos para a continuação dessa pesquisa e validação dessas hipóteses em uma tese de mestrado.
Além da revisão bibliográfica levantada sobre o tema, elaboramos também um experimento focado especificamente em geração de sinais de imagem a partir da arquitetura \ac{GAN}, onde mostramos a viabilidade de gerar números manuscritos do conjunto de dados MNIST.
Dessa forma, essa continuação da pesquisa é também resultado da nossa metodologia e experimentação no período de implementação do algoritmo estado da arte em extração de informação a priori CoDePPI.
%\vspace{12pt}
%A versão final do documento incluirá um resumo de todo o trabalho, incluindo metodologia, resultados e conclusão.
\acresetall % Manter essa linha!
\clearpage
\restoregeometry
%\chapter{Abstract}
%\noindent{\bfseries{\maiusc{\large Abstract}} }
%
%\vspace{24pt}
%The final version of this document will include an abstract. This will summarize the introduction (contextualization, objectives, justification), the methodology, the results, and the conclusion.
\acresetall % Manter essa linha!
\indice
\acresetall % Manter essa linha!
%---}}}

% |--- Lista de Símbolos, Nomenclaturas e Abreviações ---|----------------------{{{
\begin{center}
	{\bfseries{\maiusc{\large Nomenclature and Abbreviations}} }%
\end{center}

\acrodef{MRI}[MRI]{Magnetic Resonance Imaging}
\acrodef{MR}[MR]{Magnetic Resonance}
\acrodef{ANN}[ANN]{Artificial Neural Networks}
\acrodef{DL}[DL]{Deep Learning}
\acrodef{ML}[ML]{Machine Learning}
\acrodef{GAN}[GAN]{Generative Adversarial Network}
\acrodef{CNN}[CNN]{Convolutional Neural Network}
\acrodef{GPU}[GPU]{Graphics Processing Units}
\acrodef{CV}[CV]{Computer Vision}
\acrodef{NLP}[NLP]{Natural Language Processing}
\acrodef{AI}[AI]{Artificial Intelligence}
\acrodef{CS}[CS]{Compressed Sensing}
\acrodef{RIP}[RIP]{Restricted Isometry Property}
\acrodef{MLP}[MLP]{Multilayer Perceptron}
\acrodef{DFN}[DFN]{Deep Feedforward Networks}
\acrodef{SNR}[SNR]{Signal-to-Noise Ratio}
\acrodef{SSIM}[SSIM]{Structural SIMilarity}
\acrodef{PSNR}[PSNR]{Peak Signal-to-Noise Ratio}
\acrodef{NMSE}[NMSE]{Normalized Mean Squared Error}
\acrodef{MSE}[MSE]{Mean Squared Error}
\acrodef{DCGAN}[DCGAN]{Deep Convolutional GAN}
\acrodef{ReLU}[ReLU]{Rectified linear units}
\acrodef{SGD}[SGD]{Stochastic Gradient Descent}
\acrodef{IRLS}[IRLS]{Iteratively Reweighted Least Squares}
\acrodef{DCT}[DCT]{Discrete Cosine Transform}
\acrodef{CoDePPI}[CoDePPI]{Context-Dependent Probabilistic Prior Information}

\begin{acronym}
	\acro{MRI}{Magnetic Resonance Imaging}
	\acro{MR}{Magnetic Resonance}
	\acro{ANN}{Artificial Neural Networks}
	\acro{DL}{Deep Learning}
	\acro{ML}{Machine Learning}
	\acro{GAN}{Generative Adversarial Network}
	\acro{CNN}{Convolutional Neural Network}
	\acro{GPU}{Graphics Processing Units}
	\acro{CV}{Computer Vision}
	\acro{NLP}{Natural Language Processing}
	\acro{AI}{Artificial Intelligence}
    \acro{CS}{Compressed Sensing}
    \acro{RIP}{Restricted Isometry Property}
    \acro{MLP}{Multilayer Perceptron}
    \acro{DFN}{Deep Feedforward Networks}
    \acro{SNR}{Signal-to-Noise Ratio}
    \acro{SSIM}{Structural SIMilarity}
    \acro{PSNR}{Peak Signal-to-Noise Ratio}
    \acro{DCGAN}{Deep Convolutional GAN}
    \acro{ReLU}{Rectified linear units}
    \acro{SGD}{Stochastic Gradient Descent}
    \acro{IRLS}{iteratively reweighted least squares}
    \acro{DCT}{Discrete Cosine Transform}
    \acro{CoDePPI}{Context-Dependent Probabilistic Prior Information}
\end{acronym}

\clearpage
%---}}}

\pagenumbering{arabic}

%TODO documentar como serão feitas as analises na metodologia
% Gerar resultados com phantom dinamico

% |--- Introduction ---|----------------------{{{
\chapter{Introduction}

In this thesis, we introduce \ac{CoDePPI}, the best prior information extraction algorithm for \ac{MRI} reconstructions with the use of the \ac{CS} theory.
Our method \ac{CoDePPI} takes advantage of motion information across frames in a dynamic \ac{MRI} to weigh the confidence that the extracted positions are effectively part of a support structure, that is, reducing the noise introduced by applying prior information.
Our method achieves the highest reconstruction quality when compared to other prior information strategies and also is faster to compute than classical prior information due to its implementation that lacks the need to perform array elements positions lookup.
In our experiments, our approach provides better reconstruction in terms of the evaluated metrics: \ac{SNR}, \ac{PSNR}, \ac{SSIM}, \ac{NMSE}, \ac{MSE}.

Achieving higher quality with a reduced number of samples allows faster exam procedures, making \ac{MRI} cheaper, faster, and more convenient for both patients and clinics, which is our ultimate goal.

\section{Context}

\ac{MRI} is a widely used imaging modality in medical practice because of its great tissue contrast capabilities, it has evolved into the richest and most versatile biomedical imaging technique today~\cite{bryan_introduction_2009}, making \ac{MRI} the best option for medical imaging whenever it is possible to use.

However, like everything in life, there is a trade-off to consider when using \ac{MRI}.
Typically, reconstructing an \ac{MRI} is an ill-posed linear inverse task (a problem that has either none or infinite solutions in the desired class).
Problems of this nature impose a trade-off between \textit{accuracy} and \textit{speed}~\cite{kabanikhin_definitions_2008}.
The information obtained from \ac{MR} is commonly represented by individual samples in the k-space, which translates to the Fourier transform of the image to be reconstructed~\cite{miosso_phd}.
The sparse nature in \ac{MR} undersampling makes \ac{CS} theory a liable technique to use when reconstructing \ac{MRI}, hence we here elaborate a novel \ac{CS} prior information approach for better results.

\ac{CS} has been for years the state-of-art technique in \ac{MRI} reconstruction and has been improved later by the use of sparsifying pre-filtering techniques and prior information~\cite{miosso_prefiltering_2009, miosso_compressive_2009}.
\ac{CS} uses the premise that given a signal with a sparse representation in some known domain, it is possible to reconstruct the signal using limited linear measurements taken from a non-sparse representation.

There is still room for improvement in signal processing approaches for \ac{MRI} reconstruction like the one novel implementation of the prior information algorithm that we present in this thesis, but ultimately, the task of reconstructing a signal from very few extracted measurements can be most improved with \ac{ML} techniques.
Many times, classical approaches serve as the starting point where \ac{ML} models should start tackling the problem.
In this case, we propose a novel prior information implementation and also present the fundamentals for the \ac{DL} continuation along with investigation regarding the possibilities to improve the quality of this prior information even more by utilizing the so-called \ac{GAN}s.

\ac{ML} methods have been utterly developed and improved recently with the use of higher computing power derived from the invention of \ac{GPU} and other hardware improvements, allowing \ac{ANN} to come to practicality.
These \ac{ANN} models, often referenced as \ac{DL}, have become the state of art in various areas, such as \ac{CV}, \ac{NLP}, Recommendation Systems, amongst other fields~\cite{wan_regularization_nodate, devlin_bert_2019, kim_enhancing_2019}.
These fast-paced developments led to improvements in medical data processing using \ac{DL} as well.
\ac{ML} techniques can be used in several different manners to improve medical analysis, here we focus on applying \ac{GAN} in the process of attaining improved prior information to feed the \ac{CS} algorithm obtaining higher signal-to-noise ratios and faster computation procedures.

\section{Scientific Problem Definition and Proposal}

\ac{MRI} is great for high-quality tissue images and definitely one of the highest image-quality medical imaging modalities, but there are some drawbacks: \ac{MRI} exams are often very long and require the patient to be in a static position throughout the whole process, this makes the exam challenging for patients that have difficulties in keeping a still position for several minutes.
Another intrinsic complication in \ac{MRI} procedures is that it is extremely complicated to extract images from moving tissue like a beating heart or flowing blood veins as that would require an enormous amount of samples, which with current technologies used in clinics is not viable.
Algorithms that reconstruct \ac{MRI} try to tackle this sampling issue by producing the best possible quality images from the least amount of samples collected, making the exams faster and less sample-dependent.

\ac{CS} algorithms have been the state of art in \ac{MRI} reconstruction for the past few years and now with the advances of \ac{DL}, new techniques are being produced taking advantages of how \ac{ANN}s are powerful in imaging processing, especially \ac{CNN}s and more recently, \ac{GAN}s are becoming the new state of art techniques in several computer vision areas.
Most \ac{CS} contributions without the use of machine learning cite the usage of sparsifying pre-filtering techniques and prior information that have been proven to improve efficiency and achieve better reconstructions~\cite{miosso_compressive_2009, miosso_prefiltering_2009}.
More recently, the contributions in \ac{MRI} reconstruction have been more focused on deep learning approaches, with some architectures using \ac{CS} along \ac{ANN}s~\cite{yang_deep_2016, yang_dagan_2018, mardani_deep_2019, liang_deep_2019, cole_unsupervised_2020, deep_magnetic_2020}.

\ac{CS} reconstructions often use a preprocessing step called prior information which focuses on the extraction of support positions -- regions that normally would not move from a frame to another -- and when these positions are fed to the $\ell_p$-minimization algorithm (one of the possible ways to solve), the reconstructed image quality is improved significantly~\cite{miosso_compressive_2009, miosso_phd}.
Prior information is normally generated by mathematical approaches like filtering and thresholding on the images and can be leveraged from previous and next frames in the same \ac{MRI} exam to even medical records from previous scans.
These information extraction procedures oftentimes are restricted to few frames and do not take into account the nature of organs and tissues structures.
Another observation from this classical prior information extraction is that these support positions are binary, they either are part of a support position or they are not.
One could think that using a non-deterministic approach to this technique would likely correct some errors introduced in-between frames and that is exactly what happens when the extracted prior information is treated as a probabilistic support position~\cite{daniel_msc}.
Although non-deterministic do perform better than deterministic ones, they still do not consider the different level of confidence in each region of the subject -- that is, some regions are different in terms of motion, hence they should be treated as different levels of confidence, which is right what we investigate within this work.

Additionally, \ac{DL} models are constantly improving image reconstruction quality like the ones mentioned in~\cite{zbontar_fastmri_2019, knoll_fastmri_2020} and this implies that there is a lot of room for improvement towards \ac{MRI} reconstruction and analogously, we believe that prior information can be even more improved by the usage of \ac{GAN}s for prior information extraction and therefore, we also documented our research in such contexts aiming towards a more detailed continuation of this research project in a master's degree, where more experimentation can be done to validate our hypothesis.

\section{Objectives}

\subsection{General Objective}

Ultimately, the main goal behind this research can be roughly summarized in two intended contributions: faster \ac{MRI} exams and better \ac{MR} image quality.
To achieve this goal, we have set minor goals that are described more deeply further.
In terms of research, our goal is to introduce a novel context-dependent probabilistic prior information algorithm utilizing the different motions across the frames in a dynamic \ac{MRI} for higher confidence support position extraction.
The expected outcome of this algorithm is to improve reconstruction in quality and in runtime directly impacting both our main goals.
Additionally, we also present fundamentals and preliminary experiments for a sequel to this study using \ac{DL} techniques like \ac{GAN}s.

\subsection{Specific Objectives}

In order to achieve the general objective described above, we have set the following specific goals:

\begin{itemize}
    \item Implement all the \ac{CS} procedures in Python.
    \item Implement direct and indirect \ac{CS} \ac{MRI} reconstruction algorithm using undersampled \textit{k-space} measurements.
    \item Implement and validate the efficiency of the prior information technique.
    \item Demonstrate experiments with \ac{CS} using pre-filtering and classical prior information.
    \item Demonstrate our \ac{CoDePPI} algorithm and evaluate against other prior information techniques.
    % \item Test our algorithm with fastMRI samples provided by the NYU fastMRI Initiative database (fastmri.med.nyu.edu)~\cite{zbontar_fastmri_2019, knoll_fastmri_2020}.
    \item Present a case study with a dynamic \ac{MRI} using the CoDePPI algorithm to show how our technique impacts real-life exams.
\end{itemize}

%---}}}

% |--- Theory Foundation and State-of-Art ---|----------------------{{{
\chapter{MRI Concepts and Compressed Sensing State of Art}\label{chap:FT}

\section{Magnetic Resonance Imagery}

\ac{MRI} is an indirect process that produces cross-sectional images with high spatial resolution from nuclear magnetic resonances, gradient fields, and hydrogen atoms of the subject's anatomy~\cite{lauterbur_image_1973}.
The acquisition of these measurements is performed by a measuring instrument called \textit{receiver coil} and it can be done by using one receiver coil or in some cases with multiple coils~\cite{zbontar_fastmri_2019, knoll_fastmri_2020}.
These receiver coils are placed in proximity to a specific region in the subject to be imaged.
During the imaging process, the \ac{MRI} machine generates a sequence of spatially and temporally varying magnetic fields which induce the body to emit resonant electromagnetic response fields which are then measured by the receiver coil~\cite{zbontar_fastmri_2019, knoll_fastmri_2020}.

\subsection{K-space}

The k-space is the output generated by the \ac{MRI} machine scan after extracting measurements from a given subject tissue.
The k-space is represented in the spatial frequency in two or three dimensions of a subject and may also be referred to as the Fourier space.
Its representation contains an implicit sparsity that is exploited when performing undersampling~\cite{lustig_sparse_2007} and reinforces the usage of algorithms like \ac{CS} for \ac{MRI} reconstruction as \ac{CS} depends on signals that have a sparse representation on an orthonormal basis~\cite{donoho_compressed_2006}.
In essence, the k-space is the signal representation in the spatial frequency domain.

\begin{figure}[H]
  \centering
  \begin{minipage}[t]{0.475\textwidth}
    \includegraphics[width=\textwidth]{kspace_fastmri}
    \caption{Single-coil k-space points from the fully-sampled knee. Source:~\cite{zbontar_fastmri_2019, knoll_fastmri_2020}}
  \end{minipage}
  \begin{minipage}[t]{0.475\textwidth}
    \includegraphics[width=\textwidth]{knee_fastmri}
    \caption{Single-coil fully-sampled knee spatial image. Source:~\cite{zbontar_fastmri_2019, knoll_fastmri_2020}}
  \end{minipage}
\end{figure}

\subsection{Undersampling}

The time required to acquire all the measurements responses from every single atom in a subject would be extremely high, and problematic to everyone involved (patients, physicians and clinics).
The way machines can do faster \ac{MRI} is by performing \textit{undersampling}, also referred to as subsampling and sampling, when scanning the subject.

Undersampling is performed by giving the machine a known prescribed path in which it will extract measurements from the multidimensional k-space representation.
This allows machines to collect only a fraction of data measurements needed for image reconstruction hence speeding up the data acquisition process without critical quality loss.

There are some undersampling patterns to use and each has its benefits depending on several parameters, such as the subject's region extraction, the algorithm used for reconstruction, acquisition time.

In the figure below we can see some of the most used patterns.
In this research, we will focus mostly on the radial, spiral and cartesian undersampling method which we later use in our experiments.

\begin{figure}[H]
    \centering
    \includegraphics[width=150mm,scale=0.7]{sampling_trajectories}
    \caption{Undersampling patterns. \textbf{(a) Cartesian undersampling}, \textbf{(b) radial undersampling}, \textbf{(c) spiral undersampling}, \textbf{(d) isolated samples in the k-space, according to the realisation of a random process}~\cite{ye_compressed_2019}.}
\end{figure}

In \ac{MRI}, the undersampling techniques are often used to acquire $\eta$ elements where $\eta < 2\eta< N$, violating the Nyquist criterion.

\section{Compressed Sensing}

\subsection{Introduction}

Compressed sensing, often referred to as Compressive Sensing, represented a major breakthrough in signal processing when it was introduced by Donoho, Candès, Romberg, and Tao in 2004 ~\cite{donoho_compressed_2006, candes_robust_2006, candes_near-optimal_2006} since it allows sampling at a rate much lower than the Nyquist-Shannon's theorem: the sampling signal rate must be at least twice the maximum frequency present in the signal (Nyquist rate).

The idea behind the \ac{CS} theory was inspired by questioning the necessity of extracting large portions of samples when many of these very samples are discarded, exposing the inefficiency of trying to gather all the samples from the signal.

\begin{changemargin}{2cm}{2cm}
    ``\emph{Why go to so much effort to acquire \textbf{all} the data when \textbf{most} of what we get will be thrown away?
        Can we not just \textbf{directly measure} the part that will not
end up being thrown away?''~\cite{donoho_compressed_2006}}
\end{changemargin}

\ac{CS} is a powerful algorithm that implements a novel technique for the acquisition of signals of sparse or compressible nature.
\ac{CS} theory provides an accurate reconstruction of unknown sparse signals from underdetermined linear measurements $l$.
% For \ac{CS} to work, the given signal must have a sparse representation in some known domain e.g. Fourier transform, cosine transform, wavelet transform, etc.
\ac{CS} parts from the principle that if given $x$, a digital image or signal has a sparse representation in an orthonormal basis (e.g. wavelet, Fourier, \ac{DCT}, etc), then the $N$ most important coefficients in that expansion allow reconstruction with $\ell_2$ error $O(N^{1/2-1/p})$~\cite{donoho_compressed_2006}.

A signal $x$ is known to have a sparse representation if there is a deterministic and invertible matrix $T_{N \times N}$ so that the transformed vector $\hat{x}$ is composed of most of its $N$ components equal to zero $\hat{x} = Tx$~\cite{miosso_phd}.

\ac{MRI} $l$ extracted measurements from the \ac{MR} scanner correspond to some of the coefficients in $\hat{x} = Tx$.
In the \ac{MRI} context, the so-called \textit{k-space} is a sparse representation of the image to be reconstructed in the frequency domain and when it is undersampled, the Nyquist criterion is violated~\cite{lustig_sparse_2007}.
This occurs due to the fact that it is extremely expensive and practically impossible to acquire at $l = 2\eta$, as suggested by the Nyquist criterion~\cite{miosso_compressive_2009}.
These are favourable conditions for the \ac{CS} theory, hence all thousands of papers citing the usage of \ac{CS} for \ac{MRI} reconstruction and state-of-the-art results involving any application of \ac{CS} for \ac{MRI} reconstruction.

Given an $N$-dimensional complex discrete-time signal $x$, that is compressible by a linear transformation with a sparse representation, it is said that this signal $x$ can be fully reconstructed from an $l$-dimensional vector of measurements $b$ defined as $b = Mx$, where $M$ has shape $l \times N$.
Accordingly, the same equation can also be represented as $b = MT^{-1} \hat{x}$ and then as $MT^{-1} \hat{x} - b = 0$.
For \ac{CS} theory to successfully perform the reconstruction of the signal $x$, this signal needs to be sparse as mentioned before and must also conform with the incoherence condition that says that sparse signals in the transformed domain must be well distributed in the measurements domain -- that is, the rows of the measurement vector $b$ and the domain defined by $T$ must be incoherent~\cite{filipe_msc}.

As the system in the \ac{MRI} scenario is underdetermined, $l << N$, the system admits infinite solutions, therefore additional information regarding the nature of the signal is needed.
In \ac{CS} theory, this is accomplished by using an optimization algorithm with restrictions that exploit the sparsity in the domain defined by $T$.

The \ac{RIP}, introduced by Candès and Tao~\cite{candes_near-optimal_2006} confirms that the solution to the reconstruction as an optimization problem has guaranteed stability if the measurement matrix $b$ satisfies the \ac{RIP} property with respect to the sparsifying transformation~\cite{miosso_phd}.
For a signal $x$ with $\eta$ non-zero elements in the sparse representation, the matrix $A = MT^{-1}$ is said to satisfy the \ac{RIP} if and only if for any vector $v$ with dimension $N$ and a maximum of $3\eta$ non-zero elements,

\begin{equation}
    1-\epsilon \leq  \frac{\left \| MT^{-1} v \right \|_{2}}{\left \| v \right \|_2} \leq 1 + \epsilon,
\end{equation}

\noindent 
where $\epsilon$ is a tolerance constant that must use lower values in order to guarantee higher stabilities in the reconstruction procedure.

\subsection{Reconstructing with the $\ell_p$ Regularization}

The $\ell_p$-minimization is an optimization algorithm that is a rather common approach used in reconstruction procedure, especially the $\ell_1$ regularization, as it often produces better results.

Given the measurements vector $b$, the sparsest solution $\hat{x}$ must be found, which naturally leads to an optimization problem and could be solved by using the $\ell_0$-minimization for instance,

\begin{equation}\label{eq:direct_method}
    \begin{aligned}
        \hat{x}^* = arg\; min\; \left \| \hat{x}\right \|_0, \\
        s.t.\;MT^{-1}\hat{x} = b,
    \end{aligned}
\end{equation}

\noindent
The solution to \ref{eq:direct_method} is extremely resource-consuming and is not viable in most practical situations, \ac{MRI} reconstructions included.
Another possibility is to use the $\ell_1$-minimization, which is used throughout all the experiments in this thesis and is represented as

\begin{equation}\label{eq:l1_method}
    \begin{aligned}
        \hat{x}^* = arg\; min\; \left \| \hat{x}\right \|_1, \\
        s.t.\;MT^{-1}\hat{x} = b.
    \end{aligned}
\end{equation}

The $\ell_1$ approach is a good alternative to the \ac{MRI} reconstruction problem being an optimization of the convex nature and it can be solved by iterative calculation.
Another solution is the $\ell_p$ minimization which is represented by

\begin{equation}\label{eq:l1_method}
    \begin{aligned}
        \hat{x}^* = arg\;min\;\frac{1}{2} \left \| \hat{x}\right \|_p^p, \\
        s.t.\;MT^{-1}\hat{x} = b.
    \end{aligned}
\end{equation}

This optimization method was chosen for our experiments due to the better reconstruction quality it provides with the required measurements.
Reconstructing \ac{MRI} images is a non-convex problem that can be solved with different techniques like the \ac{IRLS}.
In \ac{CS} theory, the $\ell_p$ represents the $\ell_1$.

\section{Prior Information}

The application of prior information in \ac{MRI} reconstruction was first introduced with the general idea of exploiting the common information shared throughout sequential frames acquired from the \ac{MR} scan \cite{miosso_phd, miosso_compressive_2009}.
This would not only improve the reconstruction of \ac{MR} images but would also make dynamic scans more feasible as large portions of the image could be used as prior information for the reconstruction of future and previous frames.
The \ac{MRI} exam is known for requiring the patient to stand still throughout the exam so that there are not grand differences between different frames.

A dynamic cardiac \ac{MRI} for instance is naturally a situation that will be impossible for the subject to be still throughout the exam, as the heart will keep beating constantly; hence increasing the level of difficulty for the reconstruction.
Although a dynamic cardiac exam has this innate motion characteristic, it also contains crucial portions of \textit{support regions} shared between most of the frames that can be exploited to improve the reconstruction.
These support regions are essentially structures that will hold -- practically -- the same position therefore, these are elements in the image that share the same information and prior information application exploits this nature by slightly reducing the number of variables in our underdetermined system in the \ac{CS} algorithm.

\subsection{Prior Information Retrieval}

To leverage prior information for a frame -- as it was first introduced --, one could apply edge detection filters to first generate support position candidates and then extract the position of these edges and apply a $\tau$ factor to increase the value in these specific positions when running the \ac{CS} algorithm, based on an \ac{IRLS} method~\cite{miosso_compressive_2009}.
In practice, this operation tells the $\ell_p$-minimization that these values with a higher magnitude are likely values in a support position $\Phi$.
These procedures are extensively explored in the experiments documented in the experiments chapter.

Prior information was then introduced as a deterministic approach -- whether the pixel was a support position or it was not.
This approach, of course, raises a few questions: what if the subject moved slightly and the support position has been moderately altered? What if the next frame contains support positions that are getting narrower or wider? E.g. Brain \ac{MRI}s, where the cranial structure's diameter is increased or decreased frame-by-frame.
Prior information theory responds to these question with a high tolerance impact in the reconstruction.
It has been documented that using the same amount of mistaken support positions and correct ones will improve \ac{SNR} in the next frame's reconstruction~\cite{miosso_compressive_2009}.

Alternatively, a non-deterministic approach for prior information generation was proposed to achieve more robustness based on more accurate information.
This technique uses a Gaussian -- or normal -- distribution with fixed covariance matrix values for each pixel of the support positions in the space domain instead of a deterministic 0 or 1 value.
Using probabilistic support positions improves reconstruction metrics as shown in~\cite{daniel_msc}.

% In order to illustrate the idea behind prior information extraction from applying border-detection filters mentioned in
The images below are an example to represent a 15\% deterministic prior information of a sagittal head \ac{MR} scan from filters of horizon, vertical and diagonal borders detection.

\begin{figure}[H]
  \centering
  \begin{minipage}[t]{0.3\textwidth}
      \includegraphics[width=\textwidth]{filtered_head_hor}
  \end{minipage}
  \begin{minipage}[t]{0.3\textwidth}
      \includegraphics[width=\textwidth]{filtered_head_ver}
  \end{minipage}
  \begin{minipage}[t]{0.3\textwidth}
      \includegraphics[width=\textwidth]{filtered_head_diag}
  \end{minipage}
  \caption{Masks with 15\% points of deterministic prior information extraction.}
  \label{fig:det_pi}
\end{figure}


Additionally, these very same $\Phi$ positions have already been represented in terms of probabilities~\cite{daniel_msc} and they look slightly different, where the central positions of the detected edges are greater in magnitude and the edges are smaller, representing different levels of confidence.
Below we display what our horizontal low-pass filtered sagittal head image would look like in the non-deterministic approach.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.4\textwidth]{gaussian_filtered_head_hor}
    \caption{Non-deterministic prior information for the sagittal head image.}
\end{figure}

As expected, the borders look very blurry and thicker than the ones presented in the deterministic prior information \ref{fig:det_pi}.
This is useful because it allows for noise/error smoothing between frames and slices.

%---}}}

% |--- Methodology ---|----------------------{{{
\chapter{CS MRI Reconstruction Experiments using Prior Information and Preliminary Generation Experiment}

The following experiments have the purpose to elucidate how the application of pre-filtering and different prior information strategies impact \ac{MRI} reconstruction in different scenarios.
Later, we present a signal generation experiment with \ac{GAN}s mitigating the usage of this technology in further research continuation.
The generation test is a step before implementing these neural networks for prior information leverage on \ac{MRI} scans.

\section{Metrics and Evaluation Criteria}

All the documented experiments below were evaluated using a set of metrics widely used in the \ac{MRI} context to analyse image reconstruction quality.
The metrics and their respective meanings are:

\begin{itemize}
    \item \ac{PSNR}: represents the ratio between the power of the maximum magnitude value in the image and the power of corrupting noise that impacts the fidelity of the reconstruction. \ac{PSNR} is represented in dB and the greater it is, the greater image quality perceived.
    \item \ac{SSIM}: ranges from 0 to 1 and measures the similarity between two images. The closer to 1, the more the two signals are similar.
    \item \ac{SNR}: similarly to the \ac{PSNR}, the \ac{SNR} measures the level of the desired signal to the level of noise in the reconstructed signal. It is also represented in dB and the greater, the better.
    \item \ac{MSE}: measures the mean of the squares of the errors -- that is, the difference between the reconstructed signal and the ground truth. The lower the error, the better the reconstruction.
    \item \ac{NMSE}: computes the \ac{MSE} normalized by signal power. \ac{NMSE} is widely used in research papers and is the primary measure of choice in fastMRI dataset, but it contains a certain bias towards smoothness instead of sharpness~\cite{zbontar_fastmri_2019, knoll_fastmri_2020}.
\end{itemize}

\section{System Specifications}

All the mentioned experiments were conducted within the same operating system environment and with the same hardware.
We used Linux Manjaro with an Ubuntu docker image atop.
For more transparency, we here list the hardware and software used in this research:

\begin{itemize}
    \item CPU: AMD Ryzen 7 3700X 16 threads at 3.600 GHz
    \item Memory: 16GB at 3200 MHz
    \item OS: Ubuntu 20.04 LTS focal x86\_64
    \item Kernel: 5.4.114-1-MANJARO
    \item Python: 3.8.5
    \item GCC: 9.3.0
    \item NumPy: 1.20.2
    \item SciPy: 1.5.2
\end{itemize}

More specific information can be found in the \href{https://github.com/gabrielziegler3/CoDePPI}{CoDePPI GitHub repository} or by directly contacting the author and maintainer gabrielziegler3@gmail.com.

\section{1-D Direct vs Indirect $\ell_1$-Minimization}

$\ell_1$-minimization admits both direct and indirect approaches, in which there is the accuracy x resources trade-off.
The direct method often produces a higher quality reconstruction but is very memory consuming, whilst the indirect method loses a bit of quality but requires much less memory to compute the equations system.

To visualize this trade-off, we have created 200 random 1-D arrays ranging values from the standard normal distribution and have taken 10\% of data points randomly to reconstruct the whole signal using different $L$ sizes.

$L$ is the number of linear measurements extracted from the 1-D signal.
The figure below displays the first 10 signals created in the standard normal distribution range.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{first_tensignals}
    \caption{First 10 random 1-D signals}
\end{figure}

\section{2D Compressed Sensing Reconstruction with Pre Filtered Signal}

In order to test the sparsifying method of applying pre-filtering processing to the input measurements in the k-space for 2D images, we have conducted experiments some different images.
A simple 2D image of the well-known Shepp-Logan phantom~\cite{shepp_fourier_1974} for baseline is used for the first experiment and two \ac{MR} (sagittal head slice and a single-coil knee obtained from the NYU fastMRI Initiative database (fastmri.med.nyu.edu)~\cite{knoll_fastmri_2020, zbontar_fastmri_2019}) images that are considerably more complex than the phantom for the reconstruction task.

Within these images, $SNR$ improvement gained by applying sparsifying pre-filtering~\cite{miosso_phd, miosso_prefiltering_2009} can be best visualized and confirmed for these different scenarios.

\subsection{Spiral Undersampling}

We then created a phantom image with a shape of $256\times256$, hence having 65536 data points, using the phantominator python module.
Then, we simulated an undersampled phantom image by applying the spiral undersampling pattern achieving approximately 30.95\% of data points from the Fourier space which resulted in a matrix with 20285 non zero elements.
The same procedure was applied to the sagittal head image.

\begin{figure}[H]
  \centering
    \includegraphics[width=0.4\textwidth]{spiral_undersampling.png}
    \caption{A spiral undersampling trajectory with 30.95\% data points.}
\end{figure}

\subsection{Pre Filtering Sparsifying Transform}

For the pre-filtering step, $f=3$ is used where $f$ is the number of filters applied to increase sparsity in the signal to be reconstructed.
The filters are all $2\times2$ matrices and increase the sparsity in the signal from different perspectives, using more filters increases the ability to sparsify the signal.
The $3$ filtered images are then composed into one single image containing the highest gain each filter could provide given a single pixel in the image~\cite{miosso_prefiltering_2009}.
The different filters used can be better visualized from the figure below.

\begin{figure}[H]
  \centering
  \begin{minipage}[t]{0.3\textwidth}
      \includegraphics[width=\textwidth]{filter1.png}
      \caption{2-D High pass horizontal filter.}
  \end{minipage}
  \begin{minipage}[t]{0.3\textwidth}
      \includegraphics[width=\textwidth]{filter2.png}
      \caption{2-D High pass vertical filter.}
  \end{minipage}
  \begin{minipage}[t]{0.3\textwidth}
      \includegraphics[width=\textwidth]{filter3.png}
      \caption{2-D High pass diagonal filter.}
  \end{minipage}
\end{figure}

The pre-filtering method is evaluated against the zero-fill reconstruction method (used as a dummy baseline) and an $\ell_1$-minimization method without pre-filtering with the very same parameters used in the pre-filtering $\ell_1$-minimization.

\section{Dynamic Sagittal Cardiac Case Study}

In this experiment, we take a dynamic sagittal cardiac \ac{MRI} where the patient's heart is continuously beating and some other internal organs are moving throughout the exam, making each frame slightly different from one another, despite the \ac{MR} scan be still.
This is the real case where our technique can provide improved and faster results.

We start by extracting the first 10 frames from the exam video.
Then we crop all frames to $192 \times 192$ of shape in order to remove the borders from the video and focus on the important parts of the image.
Our reference image looks like this:

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{cardiac_frame1}
    \caption{First frame from the dynamic cardiac \ac{MRI} exam.}
\end{figure}

The first frame is the one that we do not possess any prior information from previous frames, hence we perform an $\ell_1$-minimization with the pre-filtering step with the same parameters and conditions established in the previous experiments, but without prior information.
From the second frame onwards, we will always have the previous frames to look into and extract prior information from support positions.
In our experiments, we use the first frame's reconstruction to generate our $\Phi$ positions for both the deterministic and context-dependent probabilistic prior information strategies used in the reconstruction of the next frame.
This allows the second reconstruction to take advantage of the knowledge available from the first reconstruction and creates a better quality reconstruction.

% For the pre-filtering steps, $f=3$ is used where $f$ is the number of filters applied to increase sparsity in the signal to be reconstructed.
% The filters are all $2\times2$ matrices and increase the sparsity in the signal from different perspectives, using more filters leverages the ability to sparsify the signal.
% The $3$ filtered images are then composed into one single image containing the highest gain each filter could provide given a single pixel in the image~\cite{miosso_prefiltering_2009}.
% The different filters used can be better seeing in the figure below.

\subsection{Radial Undersampling}

For this experiment, we decided to use a radial undersampling mask with 21.20\% of all the data points, concentrating most of the measurements in the low-frequency regions aiming to simulate the undersampled k-space provided by the \ac{MR} scan.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{radial_undersampling}
    \caption{Radial undersampling trajectory with 21.20\% of data points.}
\end{figure}

Following the same procedure as the other experiments, we also perform a zero-filled reconstruction with our undersampled k-space measurements vector $b$ in order to generate a baseline to compare with our latter results.

\subsection{Deterministic Prior Information}

The prior information step consists of creating $f$ 1-D arrays with the $\Phi$ positions of non-zero elements from the filtered images generated with the previous frames' reconstructions.
These $\Phi$ positions are then multiplied by a $\tau$ factor of 10 at each iteration in our $\ell_1$-minimization.

For both deterministic and context-dependent probabilistic prior information we have used the proportion of 6\%, which provided the best results in both prior information strategies for this specific scenario in our parameters fine-tuning experiments.

\subsection{CoDePPI: Context-Dependent Probabilistic Prior Information}

We propose a novel approach to the non-deterministic prior information extraction by using knowledge about regions in the image that are more dislocated than others to apply different variance in the normal distributions procedure.
When moving from a frame to another, portions of the image are dislocated in different degrees, that is, some regions will be more likely to contain support positions than others.
We call this approach CoDePPI: Context-Dependent Probabilistic Prior Information.

By taking this motion information into account, we segmented the image into four levels of movement -- \textit{high}, \textit{medium}, \textit{low} and \textit{close-to-zero} movement.
The regions labelled as \textit{high} movement receive a covariance matrix with a higher variance; thus assigning this region as a low-confidence support position, whilst the regions labelled as \textit{low} or \textit{close-to-zero} receive a Gaussian distribution with low variance, representing it as a highly likely support position prior information.
Similarly, the \textit{medium} labelled positions are assigned an intermediate variance for intermediate confidence.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{3d_variances}
    \caption{Example of Gaussian distributions with four different levels of variances.}
\end{figure}

In this section, we focus on the dynamic sagittal cardiac \ac{MR} and produce a manual segmentation for this case study specifically, but this could be generalized to other types of dynamic scans.
The segmentation step focuses on the different motion contexts related to the exam and subject.
In this case, we have a sagittal cardiac \ac{MR} exam in which the subject's regions with most motion visually detected were the heart region and digestive system, hence these portions of the image are marked as high and medium in terms of motion respectively.

\begin{figure}[H]
  \centering
  \begin{minipage}[t]{0.475\textwidth}
    \includegraphics[width=\textwidth]{cardiac_fullysampled}
    \caption{Fully-sampled cardiac MRI frame.}
  \end{minipage}
  \begin{minipage}[t]{0.475\textwidth}
    \includegraphics[width=\textwidth]{segmentation_mask}
    \caption{Manual segmentation: the lighter the colour, the more pixels are dislocated in the region throughout frames.}
  \end{minipage}
\end{figure}

Identifying these different regions gives us the possibility to apply different weights for each region in the image.
The regions labelled as \textit{high} movement receive a covariance matrix with a higher variance; thus assigning this region as a low-confidence support position, whilst the regions labelled as \textit{low} or \textit{close-to-zero} receive a Gaussian distribution with low variance, representing it as a support position prior information.
A \textit{weights} matrix is generated by computing an element-wise Gaussian distribution with the according covariance matrix on the segmented mask and then adding these distributions each iteration, forming accumulations on the centre of several support positions.
The regions labelled \textit{high} are specifically multiplied by $-1$ so that the confidence is represented with negative magnitudes.
After computing the \textit{weights} matrix, we expect to see distinguishably different magnitude levels, representing different confidences.

\begin{figure}[H]
  \centering
  \begin{minipage}[t]{0.475\textwidth}
    \includegraphics[width=\textwidth]{2d_weights}
    \caption{2D \textit{weights} matrix.}
  \end{minipage}
  \begin{minipage}[t]{0.475\textwidth}
    \includegraphics[width=\textwidth]{weight_surf}
    \caption{3D \textit{weights} matrix visualization.}
  \end{minipage}
\end{figure}

Inverting the normal distributions in regions annotated as \textit{high} is key to our implementation due to a \textit{bias} element-wise addition applied after the normal distributions step.
The latter step is used to normalize the magnitudes in our \textit{weight} matrix between some constraints.
As observed in the previous figures, the generated \textit{weights} matrix contains elements ranging from values below $0$ to values over \num{1e6}, we then apply a customized normalization function with a $\tau$ factor and a \textit{bias} constant to re-weight the values in this matrix $\omega(x) = (x \oslash \max x_i) \circ \tau + \beta$, where $\omega$ is our re-weighted \textit{weights} matrix, $x$ is the \textit{weights} matrix, $\tau$ is the factor by which the values are multiplied.
The $x$ division by the $\max x_i$, $\tau$ multiplication and $\beta$ addition are all element-wise operations and are often referred to as Hadamard operations.
The bias term defines $min x_i$ for every $x_i$ in the regions labelled as \textit{medium}, \textit{low} and \textit{close-to-zero} movement.

At this point, the \textit{weights} matrix is computed and re-weighted, so now this matrix is multiplied element-wise with each of the $\Phi$ masks created in the same fashion as the deterministic prior information, generating a \textit{weighted phi} mask $\kappa$ as in $\kappa = \omega \circ \Phi$.

After this operation, our $\Phi$ positions no longer holds deterministic values, but rather dispose of different levels of importance due to the \textit{weights}' values.

\begin{figure}[H]
  \centering
  \begin{minipage}[t]{0.475\textwidth}
    \includegraphics[width=\textwidth]{2d_norm_weighted_phi}
    \caption{2D \textit{weights} matrix visualization.}
  \end{minipage}
  \begin{minipage}[t]{0.475\textwidth}
    \includegraphics[width=\textwidth]{weighted_phi_surf.pdf}
    \caption{3D \textit{weights} matrix visualization.}
  \end{minipage}
\end{figure}

\subsection{$\ell_1$-minimization Adaptation}

% TODO verificar variaveis
After calculating $\kappa$, a minor modification is necessary for the $\ell_1$-minimization algorithm.
In essence, we perform an element-wise multiplication between the input undersampled signal and $\kappa$ every time before an iteration so that the prior information is applied in the computation. Hence, we have $x_i = x_i \circ \kappa_k$, where $x$ is the undersampled signal, $i$ is the element position and $k$ is our weighted $\Phi$ elements.

\subsection{Parameter Fine-tuning}

As to better compare the different reconstruction strategies applied in this case study, we leveraged the optimal parameters for the prior information in both deterministic and CoDePPI by extensively trying different combinations and evaluating each reconstruction.
After analysing the application of each combination, we decided to proceed using the following values where prior information was applied:

\begin{itemize}
    \item Prior information proportion: 6\%
    \item CoDePPI's $\tau = 1500$
    \item CoDePPI's $\beta= 900$
\end{itemize}

These values were used to achieve the best reconstructions with our methods, but might vary across different images and \ac{MR} modalities.

\section{Future Contribution Literature Review and Experiment}

\subsection{Preliminary Tests with Generative Adversarial Networks}

In order to test the usage of \ac{GAN}s for data generation and in the future use it along with \textit{prior information} for \ac{CS} systems, we have developed a \ac{GAN} capable of generating handwritten digits from 0 to 9 using the notable MNIST dataset.
The MNIST dataset contains 60,000 examples for training and 10,000 examples for testing.
The digits have been size-normalized and centred in a fixed-size image ($28\times28$ pixels) with values from 0 to 9.
For simplicity, each image has been flattened and converted into a 1-dimensional NumPy array of 784 features ($28\times28$).

The idea is to test if the neural network can output liable digits that look both readable (to the extent in which the MNIST dataset is) and also like it has been made by a human, just like the dataset itself.

Each MNIST image contains a $28\times28$ black and white image, like the following:

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.7]{mnist_sample}
    \caption{Sample of digits from MNIST}
\end{figure}

A \ac{DCGAN} was used for the experiment. A \ac{DCGAN} is an extension of the \ac{GAN}, except that it explicitly uses convolutional and convolutional-transpose layers in the discriminator and generator networks, respectively~\cite{radford_unsupervised_2016}.

\subsection{Data Transformation}

Each input image used by the \textit{dataloader} went through a computer-vision pre-processing step that includes:

\begin{itemize}
    \item Grayscale transform: convert the image to greyscale. When loaded, the MNIST digits are in RGB format with three channels. Greyscale reduces these three to one.
    \item ToTensor: convert the image to a PyTorch Tensor, with dimensions (channels, height, width). This also rescales the pixel values, from integers between 0 and 255 to floats between 0.0 and 1.0.
    \item Normalize: scale and translate the pixel values from the range 0.0, 1.0 to -1.0, 1.0. The first argument is \( \mu \) and the second argument is \( \sigma \), and the function applied to each pixel is:
    \begin{equation}
        \rho \leftarrow \frac{(\rho - \mu)}{\sigma}
    \end{equation}
\end{itemize}

\subsection{Generator Network Architecture}

The generator network architecture is implemented using PyTorch as:

\begin{itemize}
    \item A linear \textit{fully-connected} module (or layer) to map the latent space to a $7 \times7\times256 = 12544$-dimensional space that will later be undersampled several times until we reach $1\times28\times28$.
    \item An optional 1-dimensional batch normalization module
    \item A leaky ReLU module.
    \item A 2-dimensional convolutional layer with $padding=2$, $stride=1$ and $5\times5$ kernel (or filter).
    \item Two 2-dimensional transposed convolutional layers with $padding=1$, $stride=2$ and $4\times4$ kernel.
    \item Two optional 2-dimensional batch normalization modules after each 2-dimensional transposed convolutional layer.
    \item A \textit{Tanh} activation function, rescaling the images to a $[-1, 1]$ range.
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{generator}
    \caption{Generator network architecture of a 3-D image. Source:~\cite{radford_unsupervised_2016, oreilly_gan}}
\end{figure}

The latent space (random signal) input goes through each layer being upscaled until it reaches the target image dimension $28\times28$ and then fed into the discriminator network.

\subsection{Discriminator Network Architecture}

The discriminator is a \ac{CNN}-based image binary classifier network that takes an image as input and outputs a scalar probability that the given image is real or generated.
The architecture is quite similar to the Generator network, except backwards.
Here, the discriminator takes a $1\times28\times28$ input image, processes it through a series of convolutions, batch normalizations, and LeakyReLU layers, and outputs the final probability through a Sigmoid activation function.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{discriminator}
    \caption{Discriminator network architecture of a 3-D image. Source:~\cite{radford_unsupervised_2016, oreilly_gan}}
\end{figure}
%---}}}

% |--- Results ---|----------------------{{{
\chapter{CS Reconstruction and Preliminary Deep Learning Results}

In this section, we provide experiments in signal reconstruction using the \ac{CS} theory and contrast reconstruction quality of our proposed method -- context-depended probabilistic prior information --  with other \ac{CS} modalities.
Additionally, we also present the deep learning foundation for the future work we intend to proceed along with a preliminary generation experiment using \ac{GAN}s.

\section{1-D Compressed Sensing Reconstruction}

The reconstruction quality over computational resources demanded trade-off starts making a difference with $L$ size around 140, which is over 50\% of the fully sampled signal, a proportion much higher than the ones normally used in \ac{MRI} reconstruction.
This demonstrates that there is no big prejudice in using the indirect reconstruction method for very undersampled signals (close to 10\% of the data) such as the ones used in \ac{MRI}.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{1d_snr}
    \caption{SNR x \textit{L} size for direct and indirect $\ell_1$-minimization for 200 random 1D signals}
\end{figure}

\section{Compressed Sensing Reconstruction with Pre-Filtered Signal}

The results in the experiment show a huge gain of resolution in the pre-filtering method reconstructed image compared to using plain $\ell_1$-minimization.

The zero-filled reconstruction (dummy baseline) was unable to reconstruct a high fidelity image and performed very poorly in the PSNR and SNR metrics.
The $\ell_1$-minimization compressed sensing approach reconstructed the image with some noticeable noise artefacts, yet much better than the zero-filling approach.
Finally, the $\ell_1$-minimization along the usage of sparsifying pre-filtering delivered a great looking image without eye-catching artefacts and also increased the metrics largely.

\subsection{Phantom Reconstruction Evaluation}

\begin{figure}[H]
  \centering
  \begin{minipage}[t]{0.22\textwidth}
    \includegraphics[width=\textwidth]{recon_zero_fill.png}
    \caption{Zero-filled}
  \end{minipage}
  \begin{minipage}[t]{0.22\textwidth}
    \includegraphics[width=\textwidth]{recon_l1.png}
    \caption{$\ell_1$-minimization}
  \end{minipage}
  \begin{minipage}[t]{0.22\textwidth}
    \includegraphics[width=\textwidth]{recon_prefilter.png}
    \caption{$\ell_1$-minimization with pre-filtering}
  \end{minipage}
  \begin{minipage}[t]{0.22\textwidth}
    \includegraphics[width=\textwidth]{phantom_shepplogan.png}
    \caption{Shepp-Logan phantom reference image}
  \end{minipage}
\end{figure}

It is clear that reconstructing the image using the $\ell_1$-minimization certainly improves the \ac{MRI} reconstruction and the metrics reinforce that the pre-filtering step to pre-process the image is crucial to achieving higher image quality.

\begin{table}[h!]
  \begin{center}
    \begin{tabular}{l|lllll}
                                        & \textbf{PSNR}  & \textbf{SSIM}   & \textbf{SNR}  & \textbf{NMSE} & \textbf{MSE}   \\
      \hline
        \textbf{Zero-fill}                             & 16.39 & 0.26 & 4.22  & 3.77e-1 & 2.29e-2   \\
        \textbf{$\ell_1$-minimization}                       & 30.46 & 0.82 & 18.28 & 1.38e-3 & 8.99e-4   \\
        \textbf{Pre-filtering $\ell_1$-minimization}         & 76.90 & 0.99 & 64.73 & 3.36e-7 & 2.04e-8
    \end{tabular}
  \end{center}
  \caption{Phantom reconstruction metrics.}
  \label{tab:phantom-reconstruction}
\end{table}

\ac{PSNR} and \ac{SNR} are both represented in the dB scale, thus the results had to have improved by great orders of magnitude in each added step to achieve such results.

\subsection{Sagittal Head MRI Reconstruction}

Another experiment done was using a reference sagittal head image of shape $(256 \times 256)$.
The same spiral undersampling pattern with 30.95\% data points used in the phantom experiment was used here for artificial undersampling.

This image poses a harder reconstruction challenge as it is filled with more details and more complex structures than the phantom.
That said, it is clear that the undersampling pattern and amount of data points has not been sufficient to reconstruct a high fidelity image in any scenario, but the $\ell_1$-minimization with pre-filtering reconstruction looks like the winner again, reinforcing the idea that pre-filtering is a good pre-processing strategy.

\subsection{Sagittal Head Reconstruction Evaluation}

\begin{figure}[H]
  \centering
  \begin{minipage}[t]{0.22\textwidth}
    \includegraphics[width=\textwidth]{recon_head_zero_fill}
    \caption{Zero-filled}
  \end{minipage}
  \begin{minipage}[t]{0.22\textwidth}
    \includegraphics[width=\textwidth]{recon_head_l1}
    \caption{$\ell_1$-minimization}
  \end{minipage}
  \begin{minipage}[t]{0.22\textwidth}
    \includegraphics[width=\textwidth]{recon_head_prefilter}
    \caption{$\ell_1$-minimization with pre-filtering}
  \end{minipage}
  \begin{minipage}[t]{0.22\textwidth}
    \includegraphics[width=\textwidth]{head}
    \caption{Reference image}
  \end{minipage}
\end{figure}

The metrics evaluated were not affected as much as they were for the phantom experiment, but they were mostly improved by pre-filtering usage.

\begin{table}[h!]
  \begin{center}
    \begin{tabular}{l|lllll}
                                        & \textbf{PSNR}  & \textbf{SSIM}   & \textbf{SNR}  & \textbf{NMSE} & \textbf{MSE}   \\
      \hline
        \textbf{Zero-fill}                             & 14.34 & 0.35 & 5.52 & 0.28 & 2392.57   \\
        \textbf{$\ell_1$-minimization}                       & 17.54 & 0.48 & 8.73 & 0.13 & 1144.12   \\
        \textbf{Pre-filtering $\ell_1$-minimization}         & 17.79 & 0.49 & 8.97 & 0.12 & 1081.42
    \end{tabular}
  \end{center}
  \caption{Sagittal reconstruction metrics.}
  \label{tab:sagittal-reconstruction}
\end{table}

% \subsection{Knee Singlecoil Reconstruction}
%
% \begin{figure}[H]
%   \centering
%   \begin{minipage}[t]{0.22\textwidth}
%     \includegraphics[width=\textwidth]{singlecoil_knee_fullysampled.png}
%     \caption{Singlecoil knee reference image}
%   \end{minipage}
%   \begin{minipage}[t]{0.22\textwidth}
%     \includegraphics[width=\textwidth]{recon_zero_filled_knee.png}
%     \caption{Knee zero-filled reconstruction}
%   \end{minipage}
%   \begin{minipage}[t]{0.22\textwidth}
%     \includegraphics[width=\textwidth]{recon_singlecoil_knee.png}
%     \caption{Knee L1-minimization reconstruction}
%   \end{minipage}
%   \begin{minipage}[t]{0.22\textwidth}
%     \includegraphics[width=\textwidth]{recon_prefiltering_singlecoil_knee.png}
%     \caption{Knee L1-minimization with pre-filtering reconstruction}
%   \end{minipage}
% \end{figure}
%
%
% \begin{table}[h!]
%   \begin{center}
%     \begin{tabular}{l|llll}
%                                         & \textbf{PSNR}  & \textbf{SSIM}   & \textbf{SNR}  & \textbf{MSE}     \\
%       \hline
%         \textbf{Zero-fill}                             &  &  &  &  \\
%         \textbf{L1-minimization}                       &  &  &  &   \\
%         \textbf{Pre-filtering L1-minimization}         &  &  &  &
%     \end{tabular}
%   \end{center}
%   \caption{Singlecoil knee reconstruction metrics.}\label{tab:singlecoilknee-reconstruction}
% \end{table}

\section{Dynamic Sagittal Cardiac Case Study}

Our method shows significant robustness in the sagittal cardiac case study experiment, reconstructing better images with faster algorithm runtime when compared to other techniques, such as deterministic prior information and no prior information at all.

\subsection{Deterministic Prior Information}

The three prior information $\Phi$ obtained from the extraction of non-zero elements in the filtered images transformed to 2-D masks for visualization.
For this dynamic cardiac images, we used a proportion of prior information of 6\%, which in our experiments demonstrated to be the sweet spot for this image characteristics.

\begin{figure}[H]
  \centering
  \begin{minipage}[t]{0.3\textwidth}
      \includegraphics[width=\textwidth]{cardiac_phi_hor}
  \end{minipage}
  \begin{minipage}[t]{0.3\textwidth}
      \includegraphics[width=\textwidth]{cardiac_phi_ver}
  \end{minipage}
  \begin{minipage}[t]{0.3\textwidth}
      \includegraphics[width=\textwidth]{cardiac_phi_diag}
  \end{minipage}
  \caption{Deterministic prior information with 6\% data points proportion for horizontal, vertical and diagonal border detection filtering, respectively.}
\end{figure}

As we know the internal organs of the subject are moving throughout the whole exam, we instinctively know that some of the points represented in this deterministic approach will represent regions of high movement, thus they will likely introduce error to the minimization step.
This happens because the prior information values are extracted from simple image border detection techniques that are prone to the generation of noise, as they are not fine-tuned to our necessities.
After visually analysing the images above, it becomes clearer that some post-processing on this mask can improve the quality of the given prior information positions.

\subsection{Context-Dependent Probabilistic Prior Information}

\begin{figure}[H]
  \centering
  \begin{minipage}[t]{0.3\textwidth}
      \includegraphics[width=\textwidth]{weighted_cardiac_phi_hor}
  \end{minipage}
  \begin{minipage}[t]{0.3\textwidth}
      \includegraphics[width=\textwidth]{weighted_cardiac_phi_ver}
  \end{minipage}
  \begin{minipage}[t]{0.3\textwidth}
      \includegraphics[width=\textwidth]{weighted_cardiac_phi_diag}
  \end{minipage}
  \caption{CoDePPI with 6\% data points proportion for horizontal, vertical and diagonal border detection filtering, respectively.}
\end{figure}

Now that we have applied the customized CoDePPI weighting by multiplying the $\Phi$ values by our weights matrix $x$, the regions that should have high confidence of belonging to support structures in our subject are much better defined.
This definition observed in the visualizations above is also reflected in the $\ell_1$-minimization as it can be noted in the results~\ref{tab:dynamic-cardiac}.
These principles can be easily extrapolated to other \ac{MRI} modalities and even other reconstructions techniques apart from the medical segments.

\subsection{Dynamic Sagittal Cardiac Reconstruction Evaluation}

All experiments conducted for the dynamic sagittal cardiac were done to compare the different applications of prior information, thus all of the reconstructions were done with the usage of pre-filtering within the $\ell_1$-minimization context.

\begin{figure}[H]
    \centering
    \begin{minipage}{.30\linewidth}
        \subfloat[]{\label{main:a}\includegraphics[width=\textwidth]{cardiac_zerofilled_frame1}}
   %     \caption{Zero filled}
    \end{minipage}%
    \begin{minipage}{.30\linewidth}
        \subfloat[]{\label{main:b}\includegraphics[width=\textwidth]{cardiac_nopi}}
    \end{minipage}\par\medskip
    \caption{On the left: zero-filled reconstruction. On the right: no prior information reconstruction}
    \begin{minipage}{.30\linewidth}
        \subfloat[]{\label{main:b}\includegraphics[width=\textwidth]{cardiac_detpi}}
    \end{minipage}
    \begin{minipage}{.30\linewidth}
        \subfloat[]{\label{main:b}\includegraphics[width=\textwidth]{cardiac_codeppi}}
    \end{minipage}
    \begin{minipage}{.30\linewidth}
        \subfloat[]{\label{main:c}\includegraphics[width=\textwidth]{cardiac_frame1}}
    \end{minipage}
    \caption{From left to right: deterministic prior information; context-dependent probabilistic prior information; reference image}
    \label{fig:main}
\end{figure}

The images presented above are displayed from left to right in terms of best \ac{SNR}.
These images represent the different strategies we took for \ac{MRI} reconstructions and are hard to distinguish and perceive actual gain in terms of image quality, thus we should also analyse the metrics from our dynamic \ac{MR} reconstruction.

\begin{table}[H]
  \begin{center}
    \begin{tabular}{l|llllll}
      & \textbf{PSNR}  & \textbf{SSIM}   & \textbf{SNR}  & \textbf{NMSE} & \textbf{MSE} & \textbf{Runtime(s)}   \\
      \hline
        \textbf{Zero-fill}                                        & 24.42 & 0.73 & 14.43 & 3.60e-2 & 234.84 & - \\
        \textbf{No Prior Information}                             & 29.69 & 0.84 & 19.69 & 1.07e-2 & 69.83 & \textbf{1.95} \\
        \textbf{Deterministic}                                    & 29.95 & 0.83 & 19.96 & 1.01e-2 & 65.71 & 2.95 \\
        \textbf{CoDePPI (ours)}                                   & \textbf{30.15} & \textbf{0.84} & \textbf{20.16} & \textbf{9.62e-3} & \textbf{62.69} & 2.14
    \end{tabular}
  \end{center}
  \caption{Dynamic sagittal cardiac reconstruction metrics in terms of prior information usage. Best results for each evaluation represented in bold.}
  \label{tab:dynamic-cardiac}
\end{table}

The given experiments show that the CoDePPI approach to prior information leverage and usage gives more robust confidence in the support positions from a frame to another in the dynamic \ac{MR} exam studied when we analyse the metrics above.
The least impacted metric by the usage of prior information was the \ac{SSIM}, but the other image quality metrics were positively altered when we introduced the deterministic prior information and later the CoDePPI strategy.
Additionally, our method also performs the reconstruction faster than the deterministic approach, this happens because in the deterministic case, the $\ell_1$-minimization algorithm has a step where it needs to multiply all elements in the $\Phi$ positions by a value $\tau$, whilst in CoDePPI, we use NumPy's element-wise multiply the whole matrix by this $\tau$ value, discarding the need for array elements look-up that is slower.

\section{Possible \ac{DL} Integration with \ac{CoDePPI}}

Our implementation counts on the usage of signal processing techniques without any machine learning usage, but our application is intended to be evolved to a solution using \ac{DL} for prior information generation using \ac{GAN}s.
This research is to be continued in a higher degree thesis, where we would like to investigate more on two aspects: the generation of prior information support positions using \ac{GAN}s and a computer-vision system capable of segmenting the image based on levels of motion throughout the frames in a dynamic exam.

In this work, we have initiated the documentation of the \ac{DL} necessary for our next steps as well as have we also document an experiment for the generation of handwritten MNIST numbers using a \ac{GAN} to mitigate viability of our hypothesis to use generative algorithms in the near future.

\section{Artificial Neural Networks}

\subsection{Biological Inspirations}

\ac{ANN}s, as the name suggests, have been (loosely) inspired by biological neural networks (brains) from animals.
The concept of using many layers of vector-valued representation is drawn from neuroscience.
The choice of the functions $f^{(i)}(x)$ used to compute these representations is also loosely guided by neuroscientific observations about the functions that biological neurons compute~\cite{goodfellow_deep_2016}.
Another trait they share is that just like the human brain can be trained to pass forward only meaningful signals to achieve larger goals of the brain, the neurons on a neural network can be trained to pass along only useful signal~\cite{patterson_deep_2017}.

\subsection{Neuron}

The most basic unit in \ac{ANN}s is the \textit{artificial neuron}.
Neurons act as feature detectors and this is one of the advantages of deep learning techniques in contrast to classical machine learning as the \ac{ANN} is responsible for doing feature engineering and selection, and often outperform humans in this task.

These artificial neurons that are modelled mirroring the behaviour of the biological neuron as both of them are stimulated by inputs and carry some information they receive to other neurons.
Artificial neurons take in inputs $x_1, x_2,\ldots, x_n$, each and multiply them by their respective weights $w_1, w_2,\ldots, w_n$.
Then these weighted inputs are summed together producing the \textit{logit} of the artificial neuron, $z = \sum^n_{i=0} w_i x_i + b$, with $b$ being a constant number added called \textit{bias}.
After this, the logit is passed to a function $f$ in order to generate the value $y = f(z)$.

\begin{figure}[H]
    \centering
    \includegraphics[width=150mm,scale=0.7]{neuron}
    \caption{Schematic of an Artificial Neuron. Source:~\cite{quddus_machine_2018}}
\end{figure}

\subsection{Multilayer Perceptron}

\ac{DFN} or \ac{MLP}s are a type of \ac{ANN} very commonly used.
It is the foundation of many famous architectures like \ac{CNN}s.
\ac{DFN}s have an input layer followed by one or many hidden layers and a single output layer.
Each layer is fully connected to the adjacent layer.

\ac{MLP}s are computational models that flow information through the function $f$ that evaluates $x$.
The goal is to approximate some function $f*$.
For instance, a classifier $y = f * (x)$ maps an input $x$ to a category $y$.
The feedforward defines a mapping $y = f(x;\theta)$ and learns the value of the parameters $\theta$ that result in the best function approximation~\cite{goodfellow_deep_2016}.

The behaviour of an \ac{ANN} is shaped by its architecture, which describes the number of units it should have and how these units connect to each other and how complex the model is.
Often adding too much complexity to the network will lead to overfitting the training set, which occurs when the model shapes the training data too precisely and cannot generalise new data fed.

Most \ac{ANN}s are organized into rows of neurons called layers.
These layers are arranged in a chain-like structure, with each layer being a function of the layer before it.
These layers' goal is to extract \textit{representations} out of the data fed and generalize what is meaningful towards minimizing the error rate.
This architecture scheme is represented by the following equation, where $i$ is the layer index:

\begin{gather*}
    h^{(i)} = g^{(i)} (W^{(i)T}x + b^{(i)})
\end{gather*}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{mlp_architecture}
    \caption{\ac{ANN} Architecture Sample.}
\end{figure}

\subsection{Activation Functions}

Activation functions are a scalar-to-scalar function used to propagate the output of one layer's neurons forward to the next layer.
There are several types of activation functions for different purposes and network architectures.

The \ac{DCGAN} architecture used in my MNIST experiment is built using \ac{ReLU} activations in the generator network and a \textit{tanh} in the output layer.
The discriminator network uses LeakyReLU activations for all layers and a \textit{sigmoid} function for the output layer.
LeakyReLU activation functions~\cite{maas_rectier_nodate, empirical_relu_cnn} have been proven to work well for higher resolution modelling~\cite{radford_unsupervised_2016} in contrast to the usage of maxout activation functions that were first proposed in the original \ac{GAN} paper~\cite{goodfellow_generative_2014}.

\begin{table}[H]
    \centering
    \begin{tabular}{l|c|c|cc|}
      \toprule
      Name & Function & Derivative & \\
      \midrule
      \hline
      Sigmoid & $\phi(x) = \dfrac{1}{1+e^{-x}}$ &  $\phi'(x) = \phi(x)(1-\phi(x))$\\[3ex]
      TanH    & $\phi(x) = \dfrac{2}{1+e^{-2x}} - 1$ & $\phi'(x) = 1-\phi(x)^2$ \\[3ex]
      ReLU    & $\phi(x) = \begin{cases}
                           0 & x \leq 0 \\
                           x & x > 0
                         \end{cases} $
                         & $\phi'(x) = \begin{cases}
                                           0 & x \leq 0 \\
                                           1 & x > 0
                                         \end{cases} $ \\[4ex]
      Leaky ReLU & $\phi(x) = \begin{cases}
                           \alpha x & x \leq 0 \\
                           x & x > 0
                         \end{cases} $
                         & $\phi'(x) = \begin{cases}
                                           \alpha & x \leq 0 \\
                                           1 & x > 0
                                         \end{cases} $ \\
      \bottomrule
    \end{tabular}
    \caption{Activation functions and respective derivatives.}
    \label{tab:activation-functions}
\end{table}

Some of the most used and also required activation functions in the use of \ac{GAN}s and other widely used neural networks are described below.

\subsubsection{\ac{ReLU}}

The \ac{ReLU}~\cite{hahnloser_digital_2000} transform activates a node only if the input is above a certain threshold having a linear relationship with the dependent variable and outputs zero for every input below zero.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{relu}
    \caption{\ac{ReLU} activation function}
\end{figure}

\subsubsection{Leaky ReLU}

\ac{ReLU} activation functions have the ``dying ReLU'' problem, where a ReLU neuron is stuck in the negative side and always outputs $0$~\cite{trottier_2017, deep_learning_relu}.
This happens when the slope of \ac{ReLU} in the negative range is also 0, once a neuron gets negative, it is unlikely for it to recover.
These ``dead'' neurons are not playing any role in discriminating the input and are essentially useless.

To mitigate this issue within \ac{ReLU}s, LeakyReLUs are a strategy that opposed to having the function being zero when $x < 0$, it has instead a small negative slope (most times with $\alpha = 0.01$).

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{leaky_relu}
    \caption{Leaky \ac{ReLU} activation function}
\end{figure}

\subsubsection{Tanh}

Tanh is a hyperbolic trigonometric function that deals more easily with negative numbers~\cite{patterson_deep_2017}. Unlike the Sigmoid function, tanh ranges from -1 to 1.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{tanh}
    \caption{\textit{Tanh} activation function. Source:~\cite{patterson_deep_2017}}
\end{figure}

\subsubsection{Sigmoid}

Sigmoids can reduce extreme values or outliers in data without removing them, framing the input from 0 to 1 and most outputs will be close to either 0 or 1.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{sigmoid}
    \caption{Sigmoid activation function. Source:~\cite{patterson_deep_2017}}
\end{figure}

\subsection{Loss Functions}

Loss functions are used to determine how a neural network is performing on the given data.
A metric is calculated based on the error observed in the network's predictions and the model then tries to minimize this error in an optimization problem fashion.

Some of the most commonly used functions are described in the table below.

\begin{table}[H]
    \centering
    \begin{tabular}{|lccc}
      \midrule
        Mean squared error & MSE &= &$\displaystyle\frac{1}{n}\sum_{t=1}^{n}e_t^2$   \\
        \hline
        Root mean squared error & RMSE &= &$\displaystyle\sqrt{\frac{1}{n}\sum_{t=1}^{n}e_t^2}$ \\
        \hline
        Mean absolute error & MAE &= &$\displaystyle\frac{1}{n}\sum_{t=1}^{n}|e_t|$ \\
        \hline
        Mean absolute percentage error & MAPE &= &$\displaystyle\frac{100\%}{n}\sum_{t=1}^{n}\left |\frac{e_t}{y_t}\right|$\\
    \end{tabular}
    \caption{Loss functions and formulas.}
    \label{tab:loss-functions}
\end{table}

In the case of generative networks, the original \ac{GAN} paper presents a loss function called \textit{minmax}~\cite{goodfellow_generative_2014}, that is described as.

\begin{equation}
    min_Gmax_DV(D,G)= E_{x\sim P_{data}(x)}[logD(x)]+E_{z\sim pz(z)}[log(1 - D(G(z)))]
\end{equation}

Where $x$ is the input data representing an image, $D(x)$ is the discriminator network, $G(x)$ is the generator function and $z$ represents the latent vector that is mapped to data-space by $G$.
Hence, the scalar probability that the output of the generator $G$ is a real image is given by $D(G(z))$~\cite{goodfellow_generative_2014}.

\subsection{Backpropagation}

Backpropagation is a technique used to implement gradient descent in weight space for an \ac{MLP}~\cite{backpropagation_1986, werbos_1994}.
In essence, backpropagation computes the error partial derivatives of an approximating function $F(w, x)$ computed by the \ac{ANN} with respect to the weight and input vector for each training example~\cite{haykin_neural_2009}.

The development of the backpropagation algorithm is a milestone in neural networks development and research as it made computationally efficient to train \ac{MLP}s, thus confirming that \ac{ANN}s research field was filled with potential in the mid-1980s.

In order to evaluate the derivatives of the function $F(w,x)$ with respect to all the elements in the weight vector $w$ for an input vector $x = [x_1, x_2, \ldots, x_{m0}]^T$ for an \ac{MLP} with layer $l=2$, we have the following equation where $\varphi$ is the activation function, $w$ is the ordered weight vector and $x$ is the input vector fed into the \ac{MLP}~\cite{haykin_neural_2009}:

\begin{equation}
    \centering
    F(\mathbf{w}, \mathbf{x})=\sum_{j=0}^{m_{1}} w_{o j} \varphi\left(\sum_{i=0}^{m_{0}} w_{j i} x_{i}\right)
\end{equation}

\subsection{Gradient Descent}

Gradient descent is an optimization algorithm frequently used in \ac{ANN}s to find the values of coefficients of a function that minimizes a cost function.
Gradient descent can be very time consuming on large datasets due to the necessity of having a prediction for each instance in the training set.
In scenarios where there is a large number of data instances, a variation of gradient descent called \ac{SGD} can be used.
\ac{SGD} updates the coefficients for each training instance or batch instead of at the end after running through all the training set instances.

Most deep learning models are powered by the \ac{SGD} and it can be visualized as the figure below demonstrates: the function starts in a random point in the loss function and after each iteration, the \ac{SGD} calculates how it should adjust parameters in order to reach the minimal point in the loss function, hence moving towards the valley as illustrated.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{gradient_descent.jpg}
    \caption{Gradient descent example visualization. Source:~\cite{gradient_descent_2020}}
\end{figure}

\section{Generative Adversarial Networks}

\ac{GAN}s are a machine learning strategy proposed in 2014 by Ian Goodfellow~\cite{goodfellow_generative_2014} that consists of two simultaneously trained models: the \textit{Generator} $G(x)$ and the \textit{Discriminator} $D(x)$.
The generator has the role to generate fake data whilst the discriminator is trained to discern whether the given input is real or fake.

In essence, the generator takes a vector of random numbers $(z)$ as input and outputs a fake example that strives to look as close as possible to the training data pattern.
The discriminator takes an image $(x)$ as input from two sources: real examples from the training set and fake examples generated by the generator network, then the discriminator outputs a scalar probability that the image is real~\cite{langr_gans_2019}.

\ac{GAN}s play a minimax two-player game in which $D$ tries to maximize the probability to correctly classify real and fake samples ($\log D(x)$), whilst $G$ tries to minimize the chance that $D$ will correctly predict its generated outputs are fake ($\log (1-D(G(x)))$).

Ideally, this minimax game would resolve to a solution with $pg=p_{data}$, where the discriminator is incapable of distinguishing real from fake inputs.
However, \ac{GAN}s are still new neural network techniques and its convergence theory is still being highly researched and hardly reaching this point in reality~\cite{goodfellow_generative_2014}.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{gan_schema}
    \caption{\ac{GAN} training diagram. Source:~\cite{langr_gans_2019}}
\end{figure}

\subsection{Preliminary Tests with Generative Adversarial Networks}

Both discriminator losses (fake and real) start very high and quickly decreases as the generator loss curve goes up in an inverted correlated manner.
This happens especially because the generator starts by tricking the discriminator network very easily as it is naïve to determine if an image is real or generated.
Quickly the discriminator starts to detect how the data is disposed and manages to interpret the generated images are different from the training examples it is seeing.

This phenomenon exposes how bad the generator is in the first epochs and how easily the discriminator can distinguish between created and real.
Then as the epochs go by and both networks get more sophisticated, the generator starts to get better at creating the desired signal style and makes the discriminator's loss get higher again as it is observed in the loss curves below.

\begin{figure}[H]
  \centering
  \begin{minipage}[t]{0.4\textwidth}
    \includegraphics[width=\textwidth]{Discriminator_Loss_Fake}
    \caption{Discriminator fake loss over epochs}
  \end{minipage}
  \begin{minipage}[t]{0.4\textwidth}
    \includegraphics[width=\textwidth]{Discriminator_Loss_Real}
    \caption{Discriminator real loss over epochs}
  \end{minipage}
  \begin{minipage}[t]{0.4\textwidth}
    \centering
    \includegraphics[width=\textwidth]{Generator_Loss}
    \caption{Generator loss over epochs}
  \end{minipage}
\end{figure}

After 100 epochs, the \ac{DCGAN} for MNIST number generation had an exceptionally good performance when the generated images are displayed.
It is hard to tell if these are generated images or if they are part of the training set.
The generated images sometimes have a bit more blur to them, but certainly with more epochs and more training samples this could be minimized.
The following results were not cherry-picked in any manner.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{gan_generated_images}
    \caption{GAN generated MNIST digits}
\end{figure}

%---}}}

% |--- Conclusion ---|----------------------{{{
\chapter{Conclusion}

Our main goals were to implement and validate the efficiency of prior information for \ac{CS} reconstruction of \ac{MR} images along with the proposal of a novel method to improve the quality of the prior information leveraged.
These goals were defined in order to achieve faster \ac{MRI} exams and enable more feasible dynamic exams, where the motion is an innate part of the images extracted.
In this research, we have proposed an alternate approach for prior information extraction called CoDePPI -- Context-Dependent Probabilistic Prior Information -- that uses motion context from segmented images and this method proves efficient and produce better quality for the experiments we have conducted and is now the best prior information intuition to be used in classical reconstructions.

Additionally, many modules for \ac{CS} reconstruction have been developed in Python featuring the usage of undersampling techniques, $\ell_p$-minimization function, pre-filtering pre-processing, classical and CoDePPI implementation of the prior information and modules to convert and utilize fastMRI images, but these experiments were later discarded due to non-matching results with our algorithm.
All this research and development led to results that led to further hypothesis and investigations that we have initiated in this very document regarding the usage of \ac{DL} models for prior information generation and computer-vision approaches for automatic image segmentation for an automatized context detection for CoDePPI.

Several experiments were conducted for the comprehension of the \ac{CS} theory and the usage of pre-filtering and prior information techniques.
These experiments served for the development of knowledge along with the themes of \ac{CS} and \ac{MRI} reconstructions and later were used for comparisons between methods.
In these experiments, we bring several types of images and \ac{MR} modalities along with different undersampling techniques to test these methods against different situations and fully comprehend how different signals and techniques are impacted.
The latter experiments conducted were signal generation tests where we tested the results of a fully generated signal from a popular dataset to clarify that this architecture can serve as a signal generator for further prior information research.
Among the preliminary generative results, we also conducted research on \ac{DL} topics and documented the foundation theory within the same section we document the \ac{GAN}s results.

Our results conclude that the usage of pre-filtering and prior information techniques are preferred to be used within the \ac{CS} theory because they significantly improve the quality of the reconstructed images.
Additionally, our results show that our method along with pre-filtering application is the current best approach for classical \ac{CS} reconstructions of \ac{MRI}.
These results, in practical terms, lead to faster and better \ac{MRI} exams and especially more practical dynamic exams as our approach is specifically tested in a dynamic scenario which were our specific goals as mentioned before.
Better reconstruction quality not only implies that the exams could be conducted in a faster manner, but also that less energy would be consumed by the \ac{MR} machine in an exam, more people could be examined in a given period of time, the machine would need to be utilized less for a single exam and possibly leading to less maintenance as well.
Furthermore, we believe that the proposal of an improved prior information algorithm can lead other scientists to new possibilities and contributions to the process of prior information retrieval, after all, our goal was to not only create better results with a novel algorithm but also contribute to the field of \ac{MRI} that is undoubtedly very relevant to medicine and our society.

After implementing \ac{CoDePPI}, we believe that this work could be continued by applying \ac{DL} concepts in some of the steps that we developed throughout this thesis.
One way we believe could generate more improvement is by using \ac{GAN}s to generate the positions of support positions from one frame to another without the usage of classical border detection algorithms and hence, providing a more accurate and context-specific result that likely would improve \ac{CS} reconstructions.
Another way that \ac{DL} could be used to improve our work is to use an automatic image segmentation system based on the motion of regions in the image.
This way, the motion segmentations used in \ac{CoDePPI} could be generated seamlessly and that would certainly improve the speed for segmentation generation, as manual segmentations would not be needed.
We believe that both these hypothesis could be very well elaborated and tested in a graduation degree, but due to the time constraints of a bachelor degree's thesis, couldn't be further experimented in this very work.

% TODO
% Further investigation with more adequate statistics validations would be more appropriate in order to effectively assert that CoDePPI is a better solution in 100\% of the cases rather than the classical prior information algorithm presented in~\cite{miosso_compressive_2009}.

%---}}}

% |--- References ---|----------------------{{{
\renewcommand\bibname{List of References}
\bibliography{references}
%---}}}

\end{document}
%===}}}
