\documentclass[a4paper, 12pt]{fga}

% |--- Títulos, autor, banca ---|----------------------{{{
\title{} % título na forma principal
\tituloficha{Gaussian Probabilitistic Prior Information for Improved Compressed Sensing MRI Reconstruction,\\\,[Distrito Federal], 2021.} % título na forma a constar da ficha
                                                  % catalográfica (incluir mudanças de
                                                  % linha usando \\ quando necessário)
\titulocapaA{Compressed Sensing with Gaussian Probabilitistic}
\titulocapaB{Prior Information For}
\titulocapaC{Magnetic Resonance Image Reconstruction}

\titulofichadois{Generative Adversarial Network Prior Information for Improved Compressed Sensing Magnetic Resonance Imagery Reconstruction}
\author{~Gabriel Gomes Ziegler}
\nomeinvertido{Ziegler, Gabriel}
\orientador{~Cristiano Jacques Miosso, PhD}
\coorientador{~Davi Benevides Gusmão, MSc}
\data{May 2021}
\ano{2021}
\areaum{Compressed Sensing}
\areadois{Magnetic Resonance Image Reconstruction}
\areatres{\emph{Prior Information}}
\areaquatro{Non deterministic}
\endereco{gabrielziegler3@gmail.com}
\cep{CEP 72.444\-240}

\membrobancainterno{Prof. Adson Rocha, PhD}
% \membrobancaexterno{Davi Gusmão, MSc}
%---}}}

% |--- Bibliotecas utilizadas ---|----------------------{{{
\usepackage[margin=1in]{geometry}
\usepackage{setspace}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage[latin1]{inputenc}
\usepackage{enumerate}
\usepackage{xfrac}
\usepackage{color, colortbl}
\usepackage{placeins}
\usepackage[graphicx]{realboxes}
\usepackage{float}
\usepackage{subfig}
\usepackage{amsmath, amssymb}
\usepackage{rotating}
\usepackage{indentfirst}
\usepackage{multicol, blindtext, graphicx}
\usepackage[margin=0.40in,font=small,labelfont=bf,labelsep=period]{caption}
\usepackage{hyperref}
\usepackage[autostyle]{csquotes}
\hypersetup{
    colorlinks,
    citecolor=blue,
    filecolor=blue,
    linkcolor=black,
    urlcolor=blue
}

% TODO
% introducao OK
% especificar titulos (contexto, scientific problem, theory foundation)
% renomear methodology para
% metodologia: fazer introducao para metodos propostos
% L1 preliminary tests
% prefiltered measurements instead of signal
% Conclusion: next steps and schedule fazer tabela com cronograma de atividades
%
%---}}}

% |- Formato de referências (use apenas uma das 2 linhas seguintes; comente a outra) -|-{{{
\newcommand{\formatobibliografia}{numero}
%\newcommand{\formatobibliografia}{autorano}

\ifthenelse{\equal{\formatobibliografia}{numero}}{
\bibliographystyle{unsrt}
}
{}

\ifthenelse{\equal{\formatobibliografia}{autorano}}{
\usepackage{apalike}
\bibliographystyle{apalike}
}
{}
%---}}}

% |--- Espaçamento, configuração de título de seções ---|----------------------{{{
\onehalfspacing

\makeatletter
\renewcommand{\section}{\@startsection
{section}
{1}
{0mm}
{-\baselineskip}
{0.5\baselineskip}
{\large\bf}}
\makeatother

\makeatletter
\renewcommand{\subsection}{\@startsection
{subsection}
{2}
{0mm}
{-\baselineskip}
{0.5\baselineskip}
{\bf\sffamily}}
\makeatother

\makeatletter
\renewcommand{\subsubsection}{\@startsection
{subsubsection}
{3}
{0mm}
{-\baselineskip}
{0.5\baselineskip}
{\bf\sffamily}}
\makeatother

\setlength{\parindent}{20pt}
\setlength{\parskip}{12pt}
\newcommand{\spaceinitialsname}{0.4mm}
\newcommand{\porcento}{\scalebox{0.5}{~}\scalebox{0.9}{\%}}
\newcommand{\scanner}{\emph{scanner}}
\newcommand{\scanners}{\emph{scanners}}
\newcommand{\cmcubico}{${\textrm{cm}^{\scalebox{0.7}{3} }}$}
\setcounter{secnumdepth}{3}
%\setcounter{tocdepth}{3}
%---}}}

% |--- Comandos especiais ---|----------------------{{{
\newcommand{\cmquad}{${\textrm{cm}^{\scalebox{0.7}{2}} }$}
\newcommand{\mmquad}{${\textrm{mm}^{\scalebox{0.7}{2}} }$}
\newcommand{\gcmquad}{${\textrm{g}}/{\textrm{cm}^{\scalebox{0.7}{2}} }$}
\newcommand{\subsecref}[1]{Section~\ref{#1}}
\newcommand{\figref}[1]{Figure~\ref{#1}}
\newcommand{\etal}{\emph{et~al.}}
\newcommand{\Jawsonly}{{\emph{Jaws-Only}} }
\newcommand{\jawsonly}{{\emph{jaws-only}} }
\newcommand{\software}{\emph{software}}
\newcommand{\Section}[1]{\section{\textbf{#1} }}
\newcommand{\Sectionlabel}[2]{\section{\textbf{#1}\label{#2} }}
\newcommand{\percentagesignscale}{0.9}
\newcommand{\subsubsubsection}[1]{\vspace{16pt}\noindent\textbf{#1}\\[12pt]}
\newcommand{\commenttext}[1]{}
%---}}}

% |--- Diretório(s) com figuras (se desejar, inclua subdiretórios) ---|-------------{{{
\graphicspath{{figuras/}}
%---}}}

% |--- Lista de palavras que não podem ser separadas em sílabas ---|------------------{{{
\hyphenation{development results Commissioning possibility Philadelphia Devic Calculations Calculation}
%---}}}

\def\changemargin#1#2{\list{}{\rightmargin#2\leftmargin#1}\item[]}
\let\endchangemargin=\endlist
% |--- Texto principal ---|----------------------{{{
\begin{document}

\maketitle

% |--- Epígrafe, dedicatória ---|----------------------{{{
% Se desejar uma epígrafe, remova o % do início das próximas linhas (até ==============)
%\clearpage
%\hspace{1mm}
%
%\vfill
%
%\hspace{1mm}
%
%\begin{center}
%\emph{Epígrafe} \\
%Autor da epígrafe
%\end{center}
%
%\hspace{1mm}
%
%\vfill
%
%\hspace{1mm}
% ==============

% Se desejar uma dedicatória, remova o % do início das próximas linhas (até ==============)
%\clearpage
%\hspace{1mm}
%
%\vfill
%
%\begin{flushright}
%\begin{itshape}
%Texto da dedicatória.
%\end{itshape}
%\end{flushright}
% ==============
%---}}}

% |--- Agradecimentos ---|----------------------{{{
% Se desejar incluir agradecimentos, remova o % do início das próximas linhas (até ==============)
% \clearpage
%\noindent{\bfseries{\maiusc{\large Agradecimentos}} }
%
%\vspace{24pt} Agradecimentos
%
%\noindent
%\clearpage
% ==============
%---}}}

% |--- Resumo e Abstract ---|----------------------{{{
\newgeometry{bottom=0.8in, top=0.9in, left=0.9in, right=0.9in}

\noindent{\bfseries{\maiusc{\large Resumo}} }

A obtenção de imagens de ressonância magnética de alta qualidade é uma tarefa árdua pelo funcionamento das máquinas e pela complexidade dos tecidos analisados, tornando praticamente impossível a obtenção de todas as medidas do objeto de estudo em um exame de breve execução.
Para lidar com este problema, diversas técnicas foram aplicadas para gerar imagens de alta qualidade com quantidades menores de medidas, entre elas, \ac{CS} tem sido a técnica com melhores resultados e mais aprofundada no contexto de reconstrução de imagens de ressonância magnética nas últimas duas décadas.
\ac{CS} é capaz de produzir imagens de altíssima qualidade com proporções muito menores de amostragens, como 10\%,15\% do total de medidas disponíveis utilizando técnicas de \textit{undersampling} por sua boa capacidade de lidar com sinais esparsos.

O desempenho do \ac{CS} evoluiu com as contribuições de uso de filtros esparsificantes e com o uso de informação a priori.
Recentemente, o estado-da-arte veio sendo composto pelo uso de redes neurais profundas, muitas vezes em conjunto com o uso de \ac{CS} e outras de maneira singular.
Desta maneira, esta monografia tem como intuito trazer o uso de \ac{CS} com pré-filtragem e aplicar o uso de redes adversárias generativas para a extração de informação a priori.

A metodologia é composta pela utilização de formas variadas de \ac{CS} para a reconstrução de imagens comparando métodos, pré-processamentos e parâmetros, a fim de explicitar os diferentes resultados atingíveis com \ac{CS}.
Experimentos com redes adversárias generativas também foram conduzidos com o intuito de demonstrar a capacidade das mesmas para geração de sinais com alta fidelidade.

Os resultados obtidos demonstram que o uso de pré-filtragem é preferível para a reconstrução de imagens, melhorando as métricas de relação sinal-ruído (SNR), relação sinal-ruído de pico (PSNR) e similaridade estrutural (SSIM).
Os resultados das GANs mostram que estas são capazes de gerar sinais com alto teor de realismo quando comparadas as imagens criadas com as imagens de treino.
Os próximos passos da pesquisa envolvem aplicar o uso de informação a priori de forma manual e posteriormente usando GANs, comparando todas as técnicas e transformações aplicadas.
%\vspace{12pt}
%A versão final do documento incluirá um resumo de todo o trabalho, incluindo metodologia, resultados e conclusão.
\acresetall % Manter essa linha!
\clearpage
\restoregeometry
%\chapter{Abstract}
%\noindent{\bfseries{\maiusc{\large Abstract}} }
%
%\vspace{24pt}
%The final version of this document will include an abstract. This will summarize the introduction (contextualization, objectives, justification), the methodology, the results, and the conclusion.
\acresetall % Manter essa linha!
\indice
%---}}}

% |--- Lista de Símbolos, Nomenclaturas e Abreviações ---|----------------------{{{
\begin{center}
	{\bfseries{\maiusc{\large Nomenclature and Abbreviations}} }%
\end{center}

\acrodef{MRI}[MRI]{Magnetic Resonance Imaging}
\acrodef{MR}[MR]{Magnetic Resonance}
\acrodef{ANN}[ANN]{Artificial Neural Networks}
\acrodef{DL}[DL]{Deep Learning}
\acrodef{ML}[ML]{Machine Learning}
\acrodef{GAN}[GAN]{Generative Adversarial Network}
\acrodef{CNN}[CNN]{Convolutional Neural Network}
\acrodef{GPU}[GPU]{Graphics Processing Units}
\acrodef{CV}[CV]{Computer Vision}
\acrodef{NLP}[NLP]{Natural Language Processing}
\acrodef{AI}[AI]{Artificial Intelligence}
\acrodef{CS}[CS]{Compressed Sensing}
\acrodef{MLP}[MLP]{Multilayer Perceptron}
\acrodef{DFN}[DFN]{Deep Feedforward Networks}
\acrodef{SNR}[SNR]{Signal-to-Noise Ratio}
\acrodef{SSIM}[SSIM]{Structural SIMilarity}
\acrodef{PSNR}[PSNR]{Peak Signal-to-Noise Ratio}
\acrodef{NMSE}[NMSE]{Normalized Mean Squared Error}
\acrodef{MSE}[MSE]{Mean Squared Error}
\acrodef{DCGAN}[DCGAN]{Deep Convolutional GAN}
\acrodef{ReLU}[ReLU]{Rectified linear units}
\acrodef{SGD}[SGD]{Stochastic Gradient Descent}
\acrodef{IRLS}[IRLS]{Iteratively Reweighted Least Squares}

\begin{acronym}
	\acro{MRI}{Magnetic Resonance Imaging}
	\acro{MR}{Magnetic Resonance}
	\acro{ANN}{Artificial Neural Networks}
	\acro{DL}{Deep Learning}
	\acro{ML}{Machine Learning}
	\acro{GAN}{Generative Adversarial Network}
	\acro{CNN}{Convolutional Neural Network}
	\acro{GPU}{Graphics Processing Units}
	\acro{CV}{Computer Vision}
	\acro{NLP}{Natural Language Processing}
	\acro{AI}{Artificial Intelligence}
    \acro{CS}{Compressed Sensing}
    \acro{MLP}{Multilayer Perceptron}
    \acro{DFN}{Deep Feedforward Networks}
    \acro{SNR}{Signal-to-Noise Ratio}
    \acro{SSIM}{Structural SIMilarity}
    \acro{PSNR}{Peak Signal-to-Noise Ratio}
    \acro{DCGAN}{Deep Convolutional GAN}
    \acro{ReLU}{Rectified linear units}
    \acro{SGD}{Stochastic Gradient Descent}
    \acro{IRLS}{iteratively reweighted least squares}
\end{acronym}

\clearpage
%---}}}

\pagenumbering{arabic}

% |--- Introduction ---|----------------------{{{
\chapter{Introduction}

In this thesis, we propose a novel context-dependent prior information extraction algorithm that takes advantage of motion information across frames in a dynamic \ac{MRI} to weight the confidence that those extracted positions are effectively part of a support structure. 
In our experiments, our approach provides better reconstruction in terms of the evaluated metrics (\ac{SNR}, \ac{PSNR}, \ac{SSIM}, \ac{NMSE}, \ac{MSE}) and also in computation time when contrasting with classic prior information and with \ac{CS} reconstructions that use no prior information.
Achieving higher quality with a reduced number of samples allows faster exam procedures, making \ac{MRI} cheaper, faster and more convenient for both patients and clinics, which is our ultimate goal.

\section{Context}

\ac{MRI} is a widely used imaging modality in medical practice because of its great tissue contrast capabilities, it has evolved into the richest and most versatile biomedical imaging technique today~\cite{bryan_introduction_2009}, making \ac{MRI} the best option for medical imaging whenever it is possible to use.

However, like everything in life, there is a trade-off to consider when using \ac{MRI}.
Typically, reconstructing an \ac{MRI} is an ill-posed linear inverse task (a problem that has either none or infinite solutions in the desired class).
Problems of this nature impose a trade-off between \textit{accuracy} and \textit{speed}~\cite{kabanikhin_definitions_2008}.
The information obtained from \ac{MR} is commonly represented by individual samples in the k-space, which translates to the Fourier transform of the image to be reconstructed~\cite{miosso_phd}.
This \ac{MR} sampling sparse nature makes \ac{CS} a liable technique to use when reconstructing \ac{MRI}, hence we here propose a novel \ac{CS} prior information approach for better results.

\ac{CS} has been for years the state-of-art technique in \ac{MRI} reconstruction and has been improved later by the use of sparsifying pre-filtering techniques and prior information~\cite{miosso_prefiltering_2009, miosso_compressive_2009}.
\ac{CS} uses the premise that given a signal with a sparse representation in some known domain, it is possible to reconstruct the signal using limited linear measurements taken from a non-sparse representation.

There is still room for improvement in signal processing approaches for \ac{MRI} reconstruction like the one novel implementation of the prior information algorithm that we present in this thesis, but ultimately, the task of reconstructing a signal from very few extracted measurements can be mostly improved with \ac{ML} techniques.
Many times, classic-oriented approaches tend to orient where \ac{ML} models should start tackling the problem.
In this case, we propose a novel prior information implementation and also investigate possibilities to improve the quality of this prior information even more by utilizing the so-called \ac{GAN}s.

\ac{ML} methods have been utterly developed and improved recently with the use of higher computing power derived from the invention of \ac{GPU} and other hardware improvements, allowing \ac{ANN} to come to practicality.
These \ac{ANN} models, often referenced as \ac{DL}, have become the state-of-art in various areas, such as \ac{CV}, \ac{NLP}, Recommendation Systems, amongst other fields~\cite{wan_regularization_nodate, devlin_bert_2019, kim_enhancing_2019}.
These fast-paced developments led to improvements in medical data processing using \ac{DL} as well.
\ac{ML} techniques can be used in several different manners to improve medical analysis, here we focus on applying \ac{GAN} in the process of attaining improved prior information to feed the \ac{CS} algorithm obtaining higher signal-to-noise ratios and faster computation procedures.

\section{Scientific Problem Definition and Proposal}

\ac{MRI} is great for high-quality tissue images and definitely one of the highest image-quality medical imaging modalities, but there are some drawbacks: \ac{MRI} exams are often very long and require the patient to be in a static position throughout the whole process, this makes the exam challenging for patients that have difficulties in keeping a still position for several minutes.
Another intrinsic complication in \ac{MRI} procedures is that it is extremely complicated to extract images from moving tissues like a beating heart or flowing blood veins as that would require an enormous amount of samples, which with current technologies used in clinics is not viable.
Algorithms that reconstruct \ac{MRI} try to tackle this sampling issue by producing the best possible quality images from the least amount of samples collected, making the exams faster and less sample-dependent.

\ac{CS} algorithms have been the state-of-art in \ac{MRI} reconstruction for the past few years, and now with the advances of \ac{DL}, new techniques are being produced taking advantages of how \ac{ANN}s are powerful in imaging processing, especially \ac{CNN}s and, more recently, \ac{GAN}s are becoming the new state-of-art techniques in several computer vision areas.
Most \ac{CS} contributions without the use of machine learning cite the usage of sparsifying pre-filtering techniques and prior information that have been proven to improve efficiency and achieve better reconstructions~\cite{miosso_compressive_2009, miosso_prefiltering_2009}.
More recently, the contributions in \ac{MRI} reconstruction has been more focused in deep learning approaches, with some architectures using \ac{CS} along \ac{ANN}s~\cite{yang_deep_2016, yang_dagan_2018, mardani_deep_2019, liang_deep_2019, cole_unsupervised_2020, deep_magnetic_2020}.

\ac{CS} reconstructions often use a preprocessing step called prior information which focuses on the extraction of support positions -- regions that normally would not move from a frame to another -- and when these positions are fed to the $l_p$-minimization algorithm (one of the possible ways to solve), the reconstructed image quality is improved significantly~\cite{miosso_compressive_2009, miosso_phd}.
Prior information is normally generated by mathematical approaches like filtering and thresholding on the images and can be leveraged from previous and next frames in the same \ac{MRI} exam to to even medical records from previous scans.
These information extraction procedures oftentimes are restricted to few frames and do not take into account the nature of organs and tissues structures.
Another observation from this classical prior information extraction is that these support positions are binary, they either are part of a support position or they are not. 
Onee could think that using a non-deterministic approach to this technique would likely correct some errors introduced in-between frames and that is exactly what happens when the extracted prior information is treated as a probabilistic support position~\cite{daniel_msc}.
Although non-deterministic do perform better than deterministic ones, they still do not consider the different level of confidence in each region of the subject -- that is, some regions are different in terms of motion, hence they should be treated as different levels of confidence, which is right what we investigate within this work.

Additionaly, \ac{DL} models are constantly improving image reconstruction quality like the ones mentioned in~\cite{zbontar_fastmri_2019, knoll_fastmri_2020} and this implies that there is a lot of room for improvement towards \ac{MRI} and following the same logic, we believe that prior information can be even more improved by the usage of \ac{GAN}s for prior information extraction and so we also documented our research in such contexts aiming towards a more detailed research project in a master's degree, where more experimentation can be done to validate our hypothesis.

\section{Objectives}

\subsection{General Objective}

Ultimately, the main goal behind this research can be roughly summarized in two intended contributions: faster \ac{MRI} exams and better \ac{MR} image quality.
To achieve this goal, we have set minor goals that are described more deeply further.
In terms of research, our goal is to introduce a novel context-dependent non-deterministic prior information algorithm utilizing the different motion across the frames in a dynamic \ac{MRI} for higher confidence support position extraction.
The expected outcome of this algorithm is to improve reconstruction in quality and in runtime directly impacting both our main goals.
Additionaly, we also present fundamentals and preliminary experiments for a sequel to this study using \ac{DL} techniques like \ac{GAN}s.

\subsection{Specific Objectives}

In order to achieve the general objective described above, we have set the following specific goals:

\begin{itemize}
    \item Implement direct and indirect \ac{CS} \ac{MRI} reconstruction algorithm using undersampled \textit{k-space} measurements.
    \item Demonstrate experiments with \ac{CS} using pre-filtering and classical prior-information.
    \item Demonstrate our novel context-dependent prior information algorithm and compare with other prior-information techniques.
    \item Test our algorithm with fastMRI samples provided by the NYU fastMRI Initiative database (fastmri.med.nyu.edu)~\cite{zbontar_fastmri_2019, knoll_fastmri_2020}.
    \item Present a case study with a dynamic \ac{MRI} using the context-dependent prior information algorithm to show how our technique could eventually impact in real exams.
\end{itemize}

%---}}}

% |--- Theory Foundation and State-of-Art ---|----------------------{{{
\chapter{MRI Concepts and Compressed Sensing State-of-Art}\label{chap:FT}

\section{Magnetic Resonance Imagery}

\ac{MRI} is an indirect process that produces cross-sectional images with high spatial resolution from nuclear magnetic resonances, gradient fields and hydrogen atoms of the subject's anatomy~\cite{lauterbur_image_1973}.
The acquisition of these measurements is performed by a measuring instrument called \textit{receiver coil} and it can be done by using one receiver coil or in some cases with multiple coils~\cite{zbontar_fastmri_2019, knoll_fastmri_2020}.
These receiver coils are placed in proximity to a specific region in the subject to be imaged.
During the imaging process, the \ac{MRI} machine generates a sequence of spatially and temporally-varying magnetic fields which induce the body to emit resonant electromagnetic response fields which are then measured by the receiver coil~\cite{zbontar_fastmri_2019, knoll_fastmri_2020}.

\subsection{K-space}

The k-space is the output generated by the \ac{MRI} machine scan after extracting measurements from a given subject tissue.
The k-space is represented in the spatial frequency in two or three dimensions of a subject and may also be referred to as the Fourier space.
This k-space representation contains an implicit sparsity that is exploited when performing undersampling~\cite{lustig_sparse_2007} and reinforce the usage of algorithms like \ac{CS} for \ac{MRI} reconstruction as \ac{CS} depends on signals that have a sparse representation in an orthonormal basis.

\begin{figure}[H]
  \centering
  \begin{minipage}[t]{0.475\textwidth}
    \includegraphics[width=\textwidth]{kspace_fastmri}
    \caption{Single-coil k-space points from fully-sampled knee. Source:~\cite{zbontar_fastmri_2019, knoll_fastmri_2020}}
  \end{minipage}
  \begin{minipage}[t]{0.475\textwidth}
    \includegraphics[width=\textwidth]{knee_fastmri}
    \caption{Single-coil fully-sampled knee spatial image. Source:~\cite{zbontar_fastmri_2019, knoll_fastmri_2020}}
  \end{minipage}
\end{figure}

\subsection{Undersampling}

The time required to acquire all the measurements responses from every single atom in a subject would be extremely high and problematic to everyone involved (patients, doctors and clinics).
The way machines can do faster \ac{MRI} is by performing \textit{undersampling}, also referred as subsampling and sampling, when scanning the subject.

Undersampling is performed by giving the machine a known prescribed path in which it will extract measurements from the multidimensional k-space representation.
This allows machines to collect only a fraction of data measurements needed for image reconstruction hence speeding up the data acquisition process without critical quality loss.

There are some undersampling patterns to use and each has its benefits depending on several parameters, such as: the subject's region extraction, the algorithm used for reconstruction, acquisition time.

In the figure below we can see some of the most used patterns.
In this research, we will focus mostly on the radial, spiral and cartesian undersampling method which we later use in our experiments.

\begin{figure}[H]
    \centering
    \includegraphics[width=150mm,scale=0.7]{sampling_trajectories}
    \caption{Under-sampling patterns. \textbf{(a) Cartesian undersampling}, \textbf{(b) radial undersampling}, \textbf{(c) spiral undersampling}, \textbf{(d) isolated samples in the k-space, according to the realisation of a random process}~\cite{ye_compressed_2019}.}\label{visina8}
\end{figure}

In \ac{MRI}, the undersampling techniques are used to acquire $eta$ elements where $\eta < 2\eta< N$, violating the Nyquist criterion.

\section{Compressed Sensing}

\subsection{Introduction}

Compressed sensing, often referred to as Compressive Sensing, represented a major breakthrough in signal processing when it was introduced by Donoho, Candès, Romberg, and Tao in 2004 ~\cite{donoho_compressed_2006, candes_robust_2006, candes_near-optimal_2006} since it allows a sampling at a rate much lower then the Nyquist-Shannon's theorem: the sampling signal rate must be at least twice the maximum frequency present in the signal (Nyquist rate).

The idea behind the \ac{CS} theory was inspired from questioning the necessity of extracting large portions of samples when much of these very samples are discarded, exposing the inefficiency of trying to gather all signal.

\begin{changemargin}{2cm}{2cm}
``\emph{Why go to so much effort to acquire all the data when most of what we get will be thrown away?
Can we not just directly measure the part that will not
end up being thrown away?''~\cite{donoho_compressed_2006}}
\end{changemargin}

\ac{CS} is a powerful algorithm that implements a novel technique for the acquisition of signals of sparse or compressible nature.
\ac{CS} theory aims to provide accurate reconstruction of unknown sparse signals from underdetermined linear measurements $l$.
% For \ac{CS} to work, the given signal must have a sparse representation in some known domain e.g. Fourier transform, cosine transform, wavelet transform, etc.
\ac{CS} parts from the principle that if given $x$, a digital image or signal has a sparse representation in an orthonormal basis (e.g.wavelet, Fourier, cosine, etc), then the $N$ most important coefficients in that expasion allow reconstruction with $l_2$ error $O(N^{1/2-1/p})$~\cite{donoho_compressed_2006}.

A signal $x$ is known to have a sparse representation if there is a deterministic and invertible matrix $T_{N \times N}$ so that the transformed vector $\hat{x}$ is composed of most of its $N$ components equal to zero~\cite{miosso_phd}.

\begin{equation}
    \label{eqn:sparsity}
    %\overset{\sim}x = Tx
    \hat{x} = Tx
\end{equation}

\ac{MRI} $l$ extracted measurements from the \ac{MR} scanner correspond to some of the coefficients in $\hat{x} = Tx$.
In MRI context, the so-called \textit{k-space} is a sparse representation of the image to be reconstructed and when it is undersampled, the Nyquist criterion is violated~\cite{lustig_sparse_2007}.
This occurs due to the fact that it is extremely expensive to acquire at $l = 2\eta$, as suggested by the Nyquist criterion~\cite{miosso_compressive_2009}.
These are favorable conditions for the \ac{CS} theory, hence all thousands of papers citing the usage of \ac{CS} for \ac{MRI} reconstruction and state-of-the-art results involving any application of \ac{CS} for \ac{MRI} reconstruction.

% TODO
\subsection{Reconstructing using the LP Regularization}

Given an $N$-dimensional complex discrete-time signal $x$, that is compressible by a linear transformation with a sparse representation tested with~\ref{eqn:sparsity}, it is said that this signal is $x$ can be fully reconstructed from an $l$-dimensional vector of measurements defined as

\begin{equation}
    \label{eqn:measurements}
    b = Mx
\end{equation}

\section{Prior Information}

The application of prior information in \ac{MRI} reconstruction was first introduced with the general idea of exploiting the common information shared throughout sequential frames acquired from the \ac{MR} scan \cite{miosso_phd, miosso_compressive_2009}.
This would not only improve the reconstruction of \ac{MR} images but would also make dynamic scans more feasible as large portions of the image could be used as prior information for the reconstruction of future and previous frames.
The \ac{MRI} exam is known for requiring the patient to stand still throughout the exam so that there are not grand differences between different frames.

A dynamic cardiac \ac{MRI} for instance is naturally a situation that will be impossible for the subject to be still throughout the exam, as the heart will keep beating constantly; hence increasing the level of difficulty for the reconstruction.
Although a dynamic cardiac exam has this innate motion characteristic, it also contains crucial portions of \textit{support regions} shared between most of the frames that can be exploited to improve the reconstrution.
These support regions are essentially structures that will hold -- practically -- the same position therefore, these are elements in the image that share the same information and prior information application exploits this nature by slightly reducing the number of variables in our underdetermined system in the \ac{CS} algorithm.

\subsection{Prior Information Retrieval}

To leverage prior information for a frame -- as it was first introduced --, one could apply edge detection filters to first generate support position candidates and then extract the position of these edges and apply a $\tau$ factor to increase the value in these specific positions when running the \ac{CS} algorithm, based on an \ac{IRLS} method \cite{miosso_compressive_2009}.
In practice, this operation tells the $l_p$-minimization that these values with a higher magnitude are likely a values in a support position $\Phi$.
These procedures are extensively explored in the experiments documented in the experiments chapter.

Prior information was then introduced as a deterministic approach -- whether the pixel was a support position or it was not.
This approach, of course, raises a few questions: what if the subject moved slightly and the support position has been moderately altered? What if the next frame contains support positions that are getting narrower or wider? E.g. Brain \ac{MRI}s, where the cranial structure's diameter is increased or decreased frame-by-frame.
Prior information theory responds to these question with a high tolerance impact in the reconstruction.
It has been documented that using the same amount of mistaken support positions and correct ones will improve \ac{SNR} in the next frame's reconstruction. \cite{} % TODO

Alternatively, a non-deterministic approach for prior information generation was proposed to achieve more robustness based on more accurate information.
This technique uses a Gaussian -- or normal -- distribution with fixed covariance matrix value for each pixel of the support positions in the space domain instead of a deterministic 0 or 1 value.
Using probabilistic support positions improves reconstruction metrics as shown in~\cite{} %TODO.



\subsection{Future Contributions}

% TODO Mover para outro lugar
Naturally, this implies that this task could gain a lot from a specialized image segmentation system or even a non-movement region detection system, which are more elaborated in future steps section.

% TODO


\section{Artificial Neural Networks}

\subsection{Biological Inspirations}

\ac{ANN}s, as the name suggests, have been (loosely) inspired by biological neural networks (brains) from animals.
The concept of using many layers of vector-valued representation is drawn from neuroscience.
The choice of the functions $f^{(i)}(x)$ used to compute these representations is also loosely guided by neuroscientific observations about the functions that biological neurons compute~\cite{goodfellow_deep_2016}.
Another trait they share is that just like the human brain can be trained to pass forward only meaningful signals to achieve larger goals of the brain, the neurons on a neural network can be trained to pass along only useful signal~\cite{patterson_deep_2017}.

\subsection{Neuron}

The most basic unit in \ac{ANN}s is the \textit{artificial neuron}.
Neurons act as feature detectors and this is one of the advantages of deep learning techniques in contrast to classical machine learning as the \ac{ANN} is responsible for doing feature engineering and selection, and often outperform humans in this task.

These artificial neurons that are modelled mirroring the behaviour of the biological neuron as both of them are stimulated by inputs and carry some information they receive to other neurons.
Artificial neurons take in inputs $x_1, x_2,\ldots, x_n$, each and multiply them by their respective weights $w_1, w_2,\ldots, w_n$.
Then these weighted inputs are summed together producing the \textit{logit} of the artificial neuron, $z = \sum^n_{i=0} w_i x_i + b$, with $b$ being a constant number added called \textit{bias}.
After this, the logit is passed to a function $f$ in order to generate the value $y = f(z)$.

\begin{figure}[H]
    \centering
    \includegraphics[width=150mm,scale=0.7]{neuron}
    \caption{Schematic of an Artificial Neuron. Source:~\cite{quddus_machine_2018}}\label{visina8}
\end{figure}

\subsection{Multilayer Perceptron}

\ac{DFN} or \ac{MLP}s are a type of \ac{ANN} very commonly used.
It is the foundation of many famous architectures like \ac{CNN}s.
\ac{DFN}s have an input layer followed by one or many hidden layers and a single output layer.
Each layer is fully connected to the adjacent layer.

\ac{MLP}s are computational models that flow information through the function $f$ that evaluates $x$.
The goal is to approximate some function $f*$.
For instance, a classifier $y = f * (x)$ maps an input $x$ to a category $y$.
The feedforward defines a mapping $y = f(x;\theta)$ and learns the value of the parameters $\theta$ that result in the best function approximation~\cite{goodfellow_deep_2016}.

The behaviour of an \ac{ANN} is shaped by its architecture, which describes the number of units it should have and how these units connect to each other and how complex the model is.
Often adding too much complexity to the network will lead to overfitting the training set, which occurs when the model shapes the training data too precisely and cannot generalise new data fed.

Most \ac{ANN}s are organized into rows of neurons called layers.
These layers are arranged in a chain-like structure, with each layer being a function of the layer before it.
These layers' goal is to extract \textit{representations} out of the data fed and generalize what is meaningful towards minimizing the error rate.
This architecture scheme is represented by the following equation, where $i$ is the layer index:

\begin{gather*}
    h^{(i)} = g^{(i)} (W^{(i)T}x + b^{(i)})
\end{gather*}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{mlp_architecture}
    \caption{\ac{ANN} Architecture Sample.}\label{visina8}
\end{figure}

\subsection{Activation Functions}

Activation functions are a scalar-to-scalar function used to propagate the output of one layer's neurons forward to the next layer.
There are several types of activation functions for different purposes and network architectures.

The \ac{DCGAN} architecture used in my MNIST experiment is built using \ac{ReLU} activations in the generator network and a \textit{tanh} in the output layer.
The discriminator network uses LeakyReLU activations for all layers and a \textit{sigmoid} function for the output layer.
LeakyReLU activation functions~\cite{maas_rectier_nodate, empirical_relu_cnn} have been proven to work well for higher resolution modelling~\cite{radford_unsupervised_2016} in contrast to the usage of maxout activation functions that were first proposed in the original \ac{GAN} paper~\cite{goodfellow_generative_2014}.

\begin{table}[H]
    \centering
    \begin{tabular}{l|c|c|cc|}
      \toprule
      Name & Function & Derivative & \\
      \midrule
      \hline
      Sigmoid & $\phi(x) = \dfrac{1}{1+e^{-x}}$ &  $\phi'(x) = \phi(x)(1-\phi(x))$\\[3ex]
      TanH    & $\phi(x) = \dfrac{2}{1+e^{-2x}} - 1$ & $\phi'(x) = 1-\phi(x)^2$ \\[3ex]
      ReLU    & $\phi(x) = \begin{cases}
                           0 & x \leq 0 \\
                           x & x > 0
                         \end{cases} $
                         & $\phi'(x) = \begin{cases}
                                           0 & x \leq 0 \\
                                           1 & x > 0
                                         \end{cases} $ \\[4ex]
      Leaky ReLU & $\phi(x) = \begin{cases}
                           \alpha x & x \leq 0 \\
                           x & x > 0
                         \end{cases} $
                         & $\phi'(x) = \begin{cases}
                                           \alpha & x \leq 0 \\
                                           1 & x > 0
                                         \end{cases} $ \\
      \bottomrule
    \end{tabular}
    \caption{Activation functions and respective derivatives.}
    \label{tab:activation-functions}
\end{table}

Some of the most used and also required activation functions in the use of \ac{GAN}s and other widely used neural networks are described below.

\subsubsection{\ac{ReLU}}

The \ac{ReLU}~\cite{hahnloser_digital_2000} transform activates a node only if the input is above a certain threshold having a linear relationship with the dependent variable and outputs zero for every input below zero.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{relu}
    \caption{\ac{ReLU} activation function}\label{visina8}
\end{figure}

\subsubsection{Leaky ReLU}

\ac{ReLU} activation functions have the ``dying ReLU'' problem, where a ReLU neuron is stuck in the negative side and always outputs $0$~\cite{trottier_2017, deep_learning_relu}.
This happens when the slope of \ac{ReLU} in the negative range is also 0, once a neuron gets negative, it is unlikely for it to recover.
These ``dead'' neurons are not playing any role in discriminating the input and are essentially useless.

To mitigate this issue within \ac{ReLU}s, LeakyReLUs are a strategy that opposed to having the function being zero when $x < 0$, it has instead a small negative slope (most times with $\alpha = 0.01$).

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{leaky_relu}
    \caption{Leaky \ac{ReLU} activation function}\label{visina8}
\end{figure}

\subsubsection{Tanh}

Tanh is a hyperbolic trigonometric function that deals more easily with negative numbers~\cite{patterson_deep_2017}. Unlike the Sigmoid function, tanh ranges from -1 to 1.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{tanh}
    \caption{\textit{Tanh} activation function. Source:~\cite{patterson_deep_2017}}\label{visina8}
\end{figure}

\subsubsection{Sigmoid}

Sigmoids can reduce extreme values or outliers in data without removing them, framing the input from 0 to 1 and most outputs will be close to either 0 or 1.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{sigmoid}
    \caption{Sigmoid activation function. Source:~\cite{patterson_deep_2017}}\label{visina8}
\end{figure}

\subsection{Loss Functions}

Loss functions are used to determine how a neural network is performing on the given data.
A metric is calculated based on the error observed in the network's predictions and the model then tries to minimize this error in an optimization problem fashion.

Some of the most commonly used functions are described in the table below.

\begin{table}[H]
    \centering
    \begin{tabular}{|lccc}
      \midrule
        Mean squared error & MSE &= &$\displaystyle\frac{1}{n}\sum_{t=1}^{n}e_t^2$   \\
        \hline
        Root mean squared error & RMSE &= &$\displaystyle\sqrt{\frac{1}{n}\sum_{t=1}^{n}e_t^2}$ \\
        \hline
        Mean absolute error & MAE &= &$\displaystyle\frac{1}{n}\sum_{t=1}^{n}|e_t|$ \\
        \hline
        Mean absolute percentage error & MAPE &= &$\displaystyle\frac{100\%}{n}\sum_{t=1}^{n}\left |\frac{e_t}{y_t}\right|$\\
    \end{tabular}
    \caption{Loss functions and formulas.}
    \label{tab:loss-functions}
\end{table}

In the case of generative networks, the original \ac{GAN} paper presents a loss function called \textit{minmax}~\cite{goodfellow_generative_2014}, that is described as.

\begin{equation}
    min_Gmax_DV(D,G)= E_{x\sim P_{data}(x)}[logD(x)]+E_{z\sim pz(z)}[log(1 - D(G(z)))]
\end{equation}

Where $x$ is the input data representing an image, $D(x)$ is the discriminator network, $G(x)$ is the generator function and $z$ represents the latent vector that is mapped to data-space by $G$.
Hence, the scalar probability that the output of the generator $G$ is a real image is given by $D(G(z))$~\cite{goodfellow_generative_2014}.

\subsection{Backpropagation}

Backpropagation is a technique used to implement gradient descent in weight space for an \ac{MLP}~\cite{backpropagation_1986, werbos_1994}.
In essence, backpropagation computes the error partial derivatives of an approximating function $F(w, x)$ computed by the \ac{ANN} with respect to the weight and input vector for each training example~\cite{haykin_neural_2009}.

The development of the backpropagation algorithm is a milestone in neural networks development and research as it made computationally efficient to train \ac{MLP}s, thus confirming that \ac{ANN}s research field was filled with potential in the mid-1980s.

In order to evaluate the derivatives of the function $F(w,x)$ with respect to all the elements in the weight vector $w$ for an input vector $x = [x_1, x_2, \ldots, x_{m0}]^T$ for an \ac{MLP} with layer $l=2$, we have the following equation where $\varphi$ is the activation function, $w$ is the ordered weight vector and $x$ is the input vector fed into the \ac{MLP}~\cite{haykin_neural_2009}:

\begin{equation}
    \centering
    F(\mathbf{w}, \mathbf{x})=\sum_{j=0}^{m_{1}} w_{o j} \varphi\left(\sum_{i=0}^{m_{0}} w_{j i} x_{i}\right)
\end{equation}

\subsection{Gradient Descent}

Gradient descent is an optimization algorithm frequently used in \ac{ANN}s to find the values of coefficients of a function that minimizes a cost function.
Gradient descent can be very time consuming on large datasets due to the necessity of having a prediction for each instance in the training set.
In scenarios where there is a large number of data instances, a variation of gradient descent called \ac{SGD} can be used.
\ac{SGD} updates the coefficients for each training instance or batch instead of at the end after running through all the training set instances.

Most deep learning models are powered by the \ac{SGD} and it can be visualized as the figure below demonstrates: the function starts in a random point in the loss function and after each iteration, the \ac{SGD} calculates how it should adjust parameters in order to reach the minimal point in the loss function, hence moving towards the valley as illustrated.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{gradient_descent.jpg}
    \caption{Gradient descent example visualization. Source:~\cite{gradient_descent_2020}}\label{visina8}
\end{figure}

\section{Generative Adversarial Networks}

\ac{GAN}s are a machine learning strategy proposed in 2014 by Ian Goodfellow~\cite{goodfellow_generative_2014} that consists of two simultaneously trained models: the \textit{Generator} $G(x)$ and the \textit{Discriminator} $D(x)$.
The generator has the role to generate fake data whilst the discriminator is trained to discern whether the given input is real or fake.

In essence, the generator takes a vector of random numbers $(z)$ as input and outputs a fake example that strives to look as close as possible to the training data pattern.
The discriminator takes an image $(x)$ as input from two sources: real examples from the training set and fake examples generated by the generator network, then the discriminator outputs a scalar probability that the image is real~\cite{langr_gans_2019}.

\ac{GAN}s play a minimax two-player game in which $D$ tries to maximize the probability to correctly classify real and fake samples ($\log D(x)$), whilst $G$ tries to minimize the chance that $D$ will correctly predict its generated outputs are fake ($\log (1-D(G(x)))$).

Ideally, this minimax game would resolve to a solution with $pg=p_{data}$, where the discriminator is incapable of distinguishing real from fake inputs.
However, \ac{GAN}s are still new neural network techniques and its convergence theory is still being highly researched and hardly reaching this point in reality~\cite{goodfellow_generative_2014}.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{gan_schema}
    \caption{\ac{GAN} training diagram. Source:~\cite{langr_gans_2019}}\label{visina8}
\end{figure}
, 
%---}}}

% |--- Methodology ---|----------------------{{{
\chapter{Preliminary Reconstructing and Signal Generating Experiments}

The following tests have had the purpose to elucidate the application of techniques for signal reconstruction with \ac{CS} using pre-filtering pre-processing step and later with prior-information for better results.
The signal generation experiment with \ac{GAN}s is a step before implemeting these neural networks to \ac{MRI} measurements for prior-information leverage.

\section{1-D Direct vs Indirect L1-Minimization}

L1-minimization admits both direct and indirect approaches, in which there is the accuracy x resources trade-off.
The direct method often produces a higher quality reconstruction but is very memory consuming, whilst the indirect method loses a little bit of quality, but requires much less memory to compute the equations system.

To visualize this trade-off, I have created 200 random 1-D arrays ranging values from the standard normal distribution and have taken 10\% of data points randomly to reconstruct the whole signal using different $L$ sizes.

$L$ is the number of linear measurements extracted from the 1-D signal.
The figure below displays the first 10 signals created in the standard normal distribution range.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{first_tensignals}
    \caption{First 10 random 1-D signals}\label{visina8}
\end{figure}

\section{2D Compressed Sensing Reconstruction with Pre Filtered Signal}

In order to experiment the sparsifying method of applying pre-filtering processing to the input measurements in the k-space for 2D images, I have conducted experiments some different images.
A simple 2D image of the well-known Shepp-Logan phantom~\cite{shepp_fourier_1974} for baseline is used for the first experiment and two \ac{MR} (sagittal head slice and a singlecoil knee obtained from the NYU fastMRI Initiative database (fastmri.med.nyu.edu)~\cite{knoll_fastmri_2020, zbontar_fastmri_2019}) images that are considerably more complex than the phantom for the reconstruction task.

Within these images, $SNR$ improvement gained by applying sparsifying pre-filtering~\cite{miosso_phd, miosso_prefiltering_2009} can be best visualized and confirmed for these different scenarios.

\subsection{Subsampling}

I then created a phantom image with dimension of $256\times256$, hence 65536 data points, using the phantominator python module.
Then, I simulated an undersampled phantom image by applying the spiral undersampling pattern achieving approximately 30.95\% of data points from the fourier space which resulted in a matrix with 20285 non zero elements.

\begin{figure}[H]
  \centering
  \begin{minipage}[t]{0.475\textwidth}
    \includegraphics[width=\textwidth]{phantom_shepplogan.png}
    \caption{Shepp-Logan phantom reference image.}
  \end{minipage}
  \begin{minipage}[t]{0.475\textwidth}
    \includegraphics[width=\textwidth]{spiral_undersampling.png}
    \caption{Spiral undersampling method with 30.95\% data points.}
  \end{minipage}
\end{figure}

\subsection{Pre-filtering sparsifying transform}

For the pre-filtering step, $f=3$ is used where $f$ is the number of filters applied to increase sparsity in the signal to be reconstructed.
The filters are all $2\times2$ matrices and increase the sparsity in the signal from different perspectives, using more filters leverages the ability to sparsify the signal.
The $3$ filtered images are then composed into one single image containing the highest gain each filter could provide given a single pixel in the image~\cite{miosso_prefiltering_2009}.
The different filters used can be better seeing in the figure below.

\begin{figure}[H]
  \centering
  \begin{minipage}[t]{0.3\textwidth}
      \includegraphics[width=\textwidth]{filter1.png}
      \caption{2-D High pass horizontal filter.}
  \end{minipage}
  \begin{minipage}[t]{0.3\textwidth}
      \includegraphics[width=\textwidth]{filter2.png}
      \caption{2-D High pass vertical filter.}
  \end{minipage}
  \begin{minipage}[t]{0.3\textwidth}
      \includegraphics[width=\textwidth]{filter3.png}
      \caption{2-D High pass diagonal filter.}
  \end{minipage}
\end{figure}

The pre-filtering method is evaluated against the zero-fill reconstruction method (used as a dummy baseline) and an L1-minimization method without pre-filtering with the very same parameters used in the pre-filtering L1-minimization.

\section{Context-Dependent Probabilitistic Prior Information}

We propose a novel approach to the non-deterministic prior information extraction by using knowledge about regions in the image that are more dislocated than others to apply different variance in the normal distributions procedure.
When moving from a frame to another, portions of the image are dislocated in different degrees, that is, some regions will be more likely to contain support positions than others.
We call this approach Context-Dependent Probabilitistic Prior Information.

By taking this motion information into account, we segmented the image into four levels of movement -- \textit{high}, \textit{medium}, \textit{low} and \textit{close-to-zero} movement.
The regions labelled as \textit{high} movement receive a covariance matrix with a higher variance; thus assigning this region as a low-confidence support position, whilst the regions labelled as \textit{low} or \textit{close-to-zero} receive a Gaussian distribution with low variance, representing it as a highly likely support position prior information.
Similarly, the \textit{medium} labeled positions are assigned an intermediate variance for an intermediate confidence.

% TODO change to higher resolution image
\begin{figure}[H]
  \centering
  \begin{minipage}[t]{0.475\textwidth}
    \includegraphics[width=\textwidth]{cardiac_fullysampled.pdf}
    \caption{Fully-sampled cardiac MRI frame.}
  \end{minipage}
  \begin{minipage}[t]{0.475\textwidth}
    \includegraphics[width=\textwidth]{segmentation_mask.pdf}
    \caption{Manual segmentation: the lighter the colour, the more pixels are dislocated in the region throughout frames.}
  \end{minipage}
\end{figure}

Identifying these different regions gives us the possibility to apply different weights for each region in the image.
The regions labelled as \textit{high} movement receive a covariance matrix with a higher variance; thus assigning this region as a low-confidence support position, whilst the regions labelled as \textit{low} or \textit{close-to-zero} receive a Gaussian distribution with low variance, representing it as a support position prior information.
A \textit{weights} matrix is generated by computing an element-wise Gaussian distribution with the according covariance matrix on the segmented mask and then adding these distributions each iteration, forming accumulations onthe centre of several support positions.
The regions labelled \textit{high} are specifically multiplied by $-1$ so that the confidence is represented with negative magnitudes.
After computing the \textit{weights} matrix, we expect to see distinguishably different magnitude levels, representing different confidences.

\begin{figure}[H]
  \centering
  \begin{minipage}[t]{0.475\textwidth}
    \includegraphics[width=\textwidth]{2d_weights.pdf}
    \caption{2D \textit{weights} matrix visualization.}
  \end{minipage}
  \begin{minipage}[t]{0.475\textwidth}
    \includegraphics[width=\textwidth]{weight_surf.pdf}
    \caption{3D \textit{weights} matrix visualization.}
  \end{minipage}
\end{figure}

Inverting the normal distributions in regions annotated as \textit{high} is key to our implementation due to a \textit{bias} element-wise addition applied after the normal distributions step.
The latter step is used to normalize the magnitudes in our \textit{weight} matrix between some constraints.
As observed in the previous figures, the generated \textit{weights} matrix contains elements ranging from values below $0$ to values over \num{1e6}, we then apply a customized normalization function with a $\tau$ factor and a \textit{bias} constant to re-weight the values in this matrix.

\begin{equation}
    \omega(x) = (x \oslash \max x_i) \circ \tau + \beta
\end{equation}

Where $\omega$ is our re-weighted \textit{weights} matrix, $x$ is the \textit{weights} matrix, $\tau$ is the factor by which the values are multiplied and $\beta$ is the \textit{bias} term that increments all elements in the re-weighted matrix.
The bias term defines $min x_i$ for every $x_i$ in the regions labelled as \textit{medium}, \textit{low} and \textit{close-to-zero} movement.

At this point, the \textit{weights} matrix is computed and re-weighted, so now this matrix is multiplied element-wise with each of the $\Phi$ masks created in the same fashion as the deterministic prior information, generating a \textit{weighted phi} mask $\kappa$.

% TODO checar variaveis q inventei
\begin{equation}
    \kappa = \omega \circ \Phi
\end{equation}

After this operation, our $\Phi$ positions no longer holds deterministic values, but rather dispose of different levels of importance due to the \textit{weights}' values.

\begin{figure}[H]
  \centering
  \begin{minipage}[t]{0.475\textwidth}
    \includegraphics[width=\textwidth]{2d_norm_weighted_phi}
    \caption{2D \textit{weights} matrix visualization.}
  \end{minipage}
  \begin{minipage}[t]{0.475\textwidth}
    \includegraphics[width=\textwidth]{weighted_phi_surf.pdf}
    \caption{3D \textit{weights} matrix visualization.}
  \end{minipage}
\end{figure}

\subsection{\textit{L1}-minimization Adaptation}

% TODO verificar variaveis
After calculating $\kappa$, a minor modification is necessary for the \textit{L1}-minimization algorithm.
In essence, we perform an element-wise multiplication between the input and $\kappa$ every time before an iteration so that the prior information is applied in the computation.

% TODO checar variaveis q inventei
\begin{equation}
    x_i = x_i \circ \kappa_k
\end{equation}

% TODO checar variaveis q inventei
Where $x$, $i$, $k$...

\subsection{Parameter Fine-tuning}

After some fine-tuning experiments in the covariance matrices, biases and $\tau$, we have found that these are the optimal configuration for a cardiac \ac{MRI}:

% TODO insert parameters



\section{Preliminary Tests with Generative Adversarial Networks}

In order to test the usage of \ac{GAN}s for data generation and in the future use it along with \textit{prior information} for \ac{CS} systems, I have developed a \ac{GAN} capable of generating handwritten digits from 0 to 9 using the notable MNIST dataset.
The MNIST dataset contains 60,000 examples for training and 10,000 examples for testing.
The digits have been size-normalized and centred in a fixed-size image ($28\times28$ pixels) with values from 0 to 9.
For simplicity, each image has been flattened and converted into a 1-dimensional numpy array of 784 features ($28\times28$).

The idea is to test if the neural network can output liable digits that look both readable (to the extent in which the MNIST dataset is) and also like it has been made by a human, just like the dataset itself.

Each MNIST image contains a $28\times28$ black and white image, like the following:

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.7]{mnist_sample}
    \caption{Sample of digits from MNIST}\label{visina8}
\end{figure}

A \ac{DCGAN} was used for the experiment. A \ac{DCGAN} is an extension of the \ac{GAN}, except that it explicitly uses convolutional and convolutional-transpose layers in the discriminator and generator networks, respectively~\cite{radford_unsupervised_2016}.

\subsection{Data Transformation}

Each input image used by the \textit{dataloader} went through a computer-vision pre-processing step that includes:

\begin{itemize}
    \item Grayscale transform: convert the image to greyscale. When loaded, the MNIST digits are in RGB format with three channels. Greyscale reduces these three to one.
    \item ToTensor: convert the image to a PyTorch Tensor, with dimensions (channels, height, width). This also rescales the pixel values, from integers between 0 and 255 to floats between 0.0 and 1.0.
    \item Normalize: scale and translate the pixel values from the range 0.0, 1.0 to -1.0, 1.0. The first argument is \( \mu \) and the second argument is \( \sigma \), and the function applied to each pixel is:
    \begin{equation}
        \rho \leftarrow \frac{(\rho - \mu)}{\sigma}
    \end{equation}
\end{itemize}

\subsection{Generator Network Architecture}

The generator network architecture is implemented using PyTorch as:

\begin{itemize}
    \item A linear \textit{fully-connected} module (or layer) to map the latent space to a $7 \times7\times256 = 12544$-dimensional space that will later be undersampled several times until we reach $1\times28\times28$.
    \item An optional 1-dimensional batch normalization module
    \item A leaky ReLU module.
    \item A 2-dimensional convolutional layer with $padding=2$, $stride=1$ and $5\times5$ kernel (or filter).
    \item Two 2-dimensional transposed convolutional layers with $padding=1$, $stride=2$ and $4\times4$ kernel.
    \item Two optional 2-dimensional batch normalization modules after each 2-dimensional transposed convolutional layer.
    \item A \textit{Tanh} activation function, rescaling the images to a $[-1, 1]$ range.
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{generator}
    \caption{Generator network architecture of a 3-D image. Source:~\cite{radford_unsupervised_2016, oreilly_gan}}\label{visina8}
\end{figure}

The latent space (random signal) input goes through each layer being upscaled until it reaches the target image dimension $28\times28$ and then fed into the discriminator network.

\subsection{Discriminator Network Architecture}

The discriminator is a \ac{CNN}-based image binary classifier network that takes an image as input and outputs a scalar probability that the given image is real or generated.
The architecture is quite similar to the Generator network, except backwards.
Here, the discriminator takes a $1\times28\times28$ input image, processes it through a series of convolutions, batch normalizations, and LeakyReLU layers, and outputs the final probability through a Sigmoid activation function.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{discriminator}
    \caption{Discriminator network architecture of a 3-D image. Source:~\cite{radford_unsupervised_2016, oreilly_gan}}\label{visina8}
\end{figure}
%---}}}

% |--- Results ---|----------------------{{{
\chapter{Preliminary Reconstruction and Generation Results}

The results below serve to confirm the viability of the proposed methods as well as to create baselines with different images and signals that will later be compared to the fully-working \ac{GAN} + \ac{CS} proposed method.
The following preliminary results are produced with a \ac{CS} algorithm version that is still under refinement and is not yet producing the highest reconstruction standard intended for the end of the present thesis.

\section{1-D Compressed Sensing Reconstruction}

The reconstruction quality over computational resources demanded trade-off starts making a difference with $L$ size around 140, which is over 50\% of the fully sampled signal, a proportion a lot higher than the ones normally used in \ac{MRI} reconstruction.
This demonstrates that there is no big prejudice in using indirect reconstruction method for very undersampled signals (close to 10\% of the data) such as the ones used in \ac{MRI}.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{1d_snr}
    \caption{SNR x \textit{L} size for direct and indirect L1-minimization for 200 random 1D signals}\label{visina8}
\end{figure}

\section{Compressed Sensing Reconstruction with Pre Filtered Signal}

The results in the experiment show a huge gain of resolution in the pre-filtering method reconstructed image compared to the L1-minimization alone.

The zero-filled reconstruction (dummy baseline) was unable to reconstruct a high fidelity image and performed very poorly in the PSNR and SNR metrics.
The L1-minimization compressed sensing approach reconstructed the image with some noticeable noise artefacts, yet much better than the zero-filling approach.
Finally, the L1-minimization along the usage of sparsifying pre-filtering delivered a great looking image without eye-catching artefacts and also increased the metrics hugely.

\begin{figure}[H]
  \centering
  \begin{minipage}[t]{0.22\textwidth}
    \includegraphics[width=\textwidth]{recon_zero_fill.png}
    \caption{zero-filled reconstruction}
  \end{minipage}
  \begin{minipage}[t]{0.22\textwidth}
    \includegraphics[width=\textwidth]{recon_l1.png}
    \caption{L1-minimization reconstruction}
  \end{minipage}
  \begin{minipage}[t]{0.22\textwidth}
    \includegraphics[width=\textwidth]{recon_prefilter.png}
    \caption{L1-minimization with pre-filtering reconstruction}
  \end{minipage}
  \begin{minipage}[t]{0.22\textwidth}
    \includegraphics[width=\textwidth]{phantom_shepplogan.png}
    \caption{Shepp-Logan phantom reference image}
  \end{minipage}
\end{figure}

The usage of L1-minimization certainly improves the \ac{MRI} reconstruction, but the metrics reinforce how adding the pre-filtering step to preprocess the image achieves incredibly higher scores in \ac{PSNR} and \ac{SNR}.

\begin{table}[h!]
  \begin{center}
    \begin{tabular}{l|llll}
                                        & \textbf{PSNR}  & \textbf{SSIM}   & \textbf{SNR}  & \textbf{MSE}     \\
      \hline
        \textbf{Zero-fill}                             & 22.41 & 0.36 & 4.22 & 0.02     \\
        \textbf{L1-minimization}                       & 38.76 & 0.96 & 20.57 & 5.3e-5   \\
        \textbf{Pre-filtering L1-minimization}         & 91.89 & 0.99 & 73.70 & 2.5e-9
    \end{tabular}
  \end{center}
  \caption{Phantom reconstruction metrics.}
  \label{tab:phantom-reconstruction}
\end{table}

\subsection{Brain Sagittal Reconstruction}

Another experiment done was using a reference sagittal head image of shape $(256 \times 256)$.
The same spiral undersampling pattern with 30.95\% data points used in the phantom experiment was used here for artificial undersampling.

This image poses a harder reconstruction challenge as it is filled with more details and more complex structures than the phantom.
That said, it is clear that the undersampling pattern and amount of data points has not been sufficient to reconstruct a high fidelity image in any scenario, but the L1-minimization with pre-filtering reconstruction looks like the winner again, reinforcing the idea that pre-filtering is a good pre-processing strategy.

\begin{figure}[H]
  \centering
  \begin{minipage}[t]{0.22\textwidth}
    \includegraphics[width=\textwidth]{recon_head_zero_fill}
    \caption{Zero-filled reconstruction}
  \end{minipage}
  \begin{minipage}[t]{0.22\textwidth}
    \includegraphics[width=\textwidth]{recon_head_l1}
    \caption{L1-minimization reconstruction}
  \end{minipage}
  \begin{minipage}[t]{0.22\textwidth}
    \includegraphics[width=\textwidth]{recon_head_prefilter}
    \caption{L1-minimization with pre-filtering reconstruction}
  \end{minipage}
  \begin{minipage}[t]{0.22\textwidth}
    \includegraphics[width=\textwidth]{head}
    \caption{Reference image}
  \end{minipage}
\end{figure}

The metrics evaluated were not affected as much as they were for the phantom experiment, but they were mostly improved by pre-filtering usage.

\begin{table}[h!]
  \begin{center}
    \begin{tabular}{l|llll}
                                        & \textbf{PSNR}  & \textbf{SSIM}   & \textbf{SNR}  & \textbf{MSE}     \\
      \hline
        \textbf{Zero-fill}                             & 17.68 & 0.32 & 5.52 & 2392.57 \\
        \textbf{L1-minimization}                       & 21.51 & 0.43 & 8.76 & 1134.88  \\
        \textbf{Pre-filtering L1-minimization}         & 21.03 & 0.45 & 8.97 & 1081.53
    \end{tabular}
  \end{center}
  \caption{Sagittal reconstruction metrics.}
  \label{tab:sagittal-reconstruction}
\end{table}

% \subsection{Knee Singlecoil Reconstruction}
%
% \begin{figure}[H]
%   \centering
%   \begin{minipage}[t]{0.22\textwidth}
%     \includegraphics[width=\textwidth]{singlecoil_knee_fullysampled.png}
%     \caption{Singlecoil knee reference image}
%   \end{minipage}
%   \begin{minipage}[t]{0.22\textwidth}
%     \includegraphics[width=\textwidth]{recon_zero_filled_knee.png}
%     \caption{Knee zero-filled reconstruction}
%   \end{minipage}
%   \begin{minipage}[t]{0.22\textwidth}
%     \includegraphics[width=\textwidth]{recon_singlecoil_knee.png}
%     \caption{Knee L1-minimization reconstruction}
%   \end{minipage}
%   \begin{minipage}[t]{0.22\textwidth}
%     \includegraphics[width=\textwidth]{recon_prefiltering_singlecoil_knee.png}
%     \caption{Knee L1-minimization with pre-filtering reconstruction}
%   \end{minipage}
% \end{figure}
%
%
% \begin{table}[h!]
%   \begin{center}
%     \begin{tabular}{l|llll}
%                                         & \textbf{PSNR}  & \textbf{SSIM}   & \textbf{SNR}  & \textbf{MSE}     \\
%       \hline
%         \textbf{Zero-fill}                             &  &  &  &  \\
%         \textbf{L1-minimization}                       &  &  &  &   \\
%         \textbf{Pre-filtering L1-minimization}         &  &  &  &
%     \end{tabular}
%   \end{center}
%   \caption{Singlecoil knee reconstruction metrics.}\label{tab:singlecoilknee-reconstruction}
% \end{table}

\subsection{Prior Information Cardiac Sagittal Reconstruction}

In order to test if the extracted prior information from one frame is relevant and how it impacts the reconstruction of the next frame, we have tested reconstructing frames from a dynamic cardiac \ac{MRI} exam.
The extracted frames are $(n \times n)$ in shape 

% TODO change images
\begin{figure}[H]
  \centering
  \begin{minipage}[t]{0.22\textwidth}
    \includegraphics[width=\textwidth]{cardiac_fullysampled}
    \caption{Dynamic cardiac zero-filled reconstruction}
  \end{minipage}
  \begin{minipage}[t]{0.22\textwidth}
    \includegraphics[width=\textwidth]{cardiac_fullysampled}
    \caption{Dynamic cardiac L1-minimization reconstruction}
  \end{minipage}
  \begin{minipage}[t]{0.22\textwidth}
    \includegraphics[width=\textwidth]{cardiac_fullysampled}
    \caption{Dynamic cardiac L1-minimization with pre-filtering reconstruction}
  \end{minipage}
  \begin{minipage}[t]{0.22\textwidth}
    \includegraphics[width=\textwidth]{cardiac_fullysampled}
    \caption{Dynamic cardiac reference image}
  \end{minipage}
\end{figure}


% TODO fill table
\begin{table}[h!]
  \begin{center}
    \begin{tabular}{l|llll}
                                        & \textbf{PSNR}  & \textbf{SSIM}   & \textbf{SNR}  & \textbf{MSE}     \\
      \hline
        \textbf{Zero-fill}                             & 17.68 & 0.32 & 5.52 & 2392.57 \\
        \textbf{L1-minimization}                       & 21.51 & 0.43 & 8.76 & 1134.88  \\
        \textbf{Pre-filtering L1-minimization}         & 21.03 & 0.45 & 8.97 & 1081.53
    \end{tabular}
  \end{center}
  \caption{Sagittal reconstruction metrics.}
  \label{tab:sagittal-reconstruction}
\end{table}


\section{Preliminary Tests with Generative Adversarial Networks}

Both discriminator losses (fake and real) start very high and quickly decreases as the generator loss curve goes up in an invertedly correlated manner.
This happens especially because the generator starts by tricking the discriminator network very easily as it is naïve to determine if an image is real or generated.
Quickly the discriminator starts to detect how the data is disposed and manages to interpret the generated images are different from the training examples it is seeing.

This phenomenom exposes how bad the generator is in the first epochs and how easily the discriminator can distinguish between created and real.
Then as the epochs go by and both networks get more sophisticated, the generator starts to get better at creating the desired signal style and makes the discriminator's loss get higher again as it is observed in the loss curves below.

\begin{figure}[H]
  \centering
  \begin{minipage}[t]{0.4\textwidth}
    \includegraphics[width=\textwidth]{Discriminator_Loss_Fake}
    \caption{Discriminator fake loss over epochs}
  \end{minipage}
  \begin{minipage}[t]{0.4\textwidth}
    \includegraphics[width=\textwidth]{Discriminator_Loss_Real}
    \caption{Discriminator real loss over epochs}
  \end{minipage}
  \begin{minipage}[t]{0.4\textwidth}
    \centering
    \includegraphics[width=\textwidth]{Generator_Loss}
    \caption{Generator loss over epochs}
  \end{minipage}
\end{figure}

After 100 epochs, the \ac{DCGAN} for MNIST number generation had an exceptionally good performance when the generated images are displayed.
It is hard to tell if these are generated images or if they are part of the training set.
The generated images sometimes have a bit more blur to them, but certainly with more epochs and more training samples this could be minimized.
The following results were not cherry-picked in any manner.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{gan_generated_images}
    \caption{GAN generated MNIST digits}
\end{figure}

%---}}}

% |--- Conclusion ---|----------------------{{{
\chapter{Conclusion}

The next steps in this thesis involve using prior-information for best \ac{CS} results and training a \ac{GAN} for prior information leverage aiming for better resolution reconstructions and dynamic \ac{MRI}.

The cronogram has been split in four major activities: prior information with \ac{CS}, \ac{GAN} architecture design and implementation, experiment \ac{GAN} for prior information generation and evaluate \ac{GAN} results for image reconstrutions.
These tasks have been organized within the expected for each task to be well done and documented.
There is not yet an official date for the conclusion of this thesis, but the tasks have more than enough time to be well developed and evaluated.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{cronogram}
    \caption{Activies cronogram}
\end{figure}
%---}}}

% |--- References ---|----------------------{{{
\renewcommand\bibname{List of References}
\bibliography{references}
%---}}}

\end{document}
%===}}}
